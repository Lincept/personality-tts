"""
ä¸»æµ‹è¯•è„šæœ¬ - LLM + TTS é›†æˆæµ‹è¯•
æ”¯æŒå®æ—¶æµå¼å¯¹è¯å’Œå®æ—¶è¯­éŸ³æ’­æ”¾
é‡‡ç”¨ä¸¤é˜¶æ®µç»“æ„åŒ–è¾“å‡ºè®°å¿†ç®¡ç†æ–¹æ¡ˆ
æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼ï¼š
- æ–‡å­—å¯¹è¯æ¨¡å¼ï¼šä½ æ‰“å­—ï¼ŒAI è¯´è¯
- è¯­éŸ³å¯¹è¯æ¨¡å¼ï¼šä½ è¯´è¯ï¼ŒAI è¯´è¯ï¼ˆæ”¯æŒ AEC å›å£°æ¶ˆé™¤ï¼‰
"""
import os
import sys
import json
import logging
import time
import argparse
import threading
from typing import Optional

from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.llm.llm_client import LLMClient
from src.tts.qwen3_realtime_tts import Qwen3RealtimeTTS
from src.tts.volcengine_realtime_tts import VolcengineRealtimeTTS
from src.audio.pyaudio_player import PyAudioStreamPlayer
from src.config_loader import ConfigLoader
from src.realtime_pipeline import RealtimeStreamingPipeline
from src.voice_assistant_prompt import VoiceAssistantPrompt
from src.role_loader import RoleLoader
from src.memory.mem0_manager import Mem0Manager
from src.memory.memory_chat import MemoryEnhancedChat
from src.asr import DashScopeASR, AudioInput, InterruptController
from src.asr.aec_processor import SimpleAEC

def _parse_log_level(value: str) -> Optional[int]:
    """Parse log level from env; return None for 'OFF' (disable logging)."""
    if value is None:
        return logging.INFO
    v = str(value).strip().upper()
    if v in {"OFF", "NONE", "DISABLE", "FALSE", "0"}:
        return None
    if v.isdigit():
        return int(v)
    return getattr(logging, v, logging.INFO)


# å°½æ—©åŠ è½½ .envï¼Œä¿è¯ import é˜¶æ®µå°±èƒ½æŒ‰ç¯å¢ƒå˜é‡æ§åˆ¶æ—¥å¿—
load_dotenv()

_level = _parse_log_level(os.getenv("PTTS_LOG_LEVEL"))
if _level is None:
    logging.disable(logging.CRITICAL)
else:
    # force=True ç¡®ä¿å³ä½¿å…¶å®ƒæ¨¡å—å·²é…ç½®è¿‡ handler ä¹Ÿèƒ½æŒ‰ç¯å¢ƒå˜é‡é‡æ–°é…ç½®
    logging.basicConfig(level=_level, force=True)
    root_logger = logging.getLogger()
    for h in root_logger.handlers:
        h.setLevel(_level)

    # å¸¸è§å™ªå£°æ¥æºï¼šHTTP å®¢æˆ·ç«¯ã€WebSocketã€ä»¥åŠæœ¬é¡¹ç›®çš„ memory_chat
    for name in ("httpx", "httpcore", "websocket", "websockets", "memory_chat"):
        logging.getLogger(name).setLevel(_level)


class VoiceInteractiveMode:
    """è¯­éŸ³äº¤äº’æ¨¡å¼ - æ”¯æŒè¯­éŸ³è¾“å…¥å’Œæ‰“æ–­ï¼Œé›†æˆ AEC å›å£°æ¶ˆé™¤"""

    def __init__(self, enable_aec: bool = True, device_index: Optional[int] = None, asr_model: str = "paraformer-realtime-v2", role_config: dict = None):
        """
        åˆå§‹åŒ–è¯­éŸ³äº¤äº’æ¨¡å¼

        Args:
            enable_aec: æ˜¯å¦å¯ç”¨ AECï¼ˆå›å£°æ¶ˆé™¤ï¼‰- éœ€è¦é…ç½®èšåˆè®¾å¤‡
            device_index: éŸ³é¢‘è®¾å¤‡ç´¢å¼•ï¼ˆèšåˆè®¾å¤‡çš„ç´¢å¼•ï¼Œå¯ç”¨ AEC æ—¶å¿…é¡»æä¾›ï¼‰
            asr_model: ASR æ¨¡å‹é€‰æ‹©
                - "paraformer-realtime-v2": Paraformer å®æ—¶æ¨¡å‹ v2ï¼ˆæ¨èï¼Œå‡†ç¡®åº¦é«˜ï¼‰
                - "fun-asr-realtime-2025-11-07": FunASR 2025 ç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼‰
            role_config: è§’è‰²é…ç½®
        """
        # åŠ è½½ç¯å¢ƒå˜é‡
        env_vars = load_env_file()
        api_key = (
            env_vars.get('QWEN3_API_KEY') or os.getenv('QWEN3_API_KEY') or
            env_vars.get('DASHSCOPE_API_KEY') or os.getenv('DASHSCOPE_API_KEY')
        )

        if not api_key:
            raise ValueError(
                'æœªæ‰¾åˆ° DashScope API Keyï¼Œè¯·åœ¨ .env ä¸­è®¾ç½® QWEN3_API_KEY æˆ– DASHSCOPE_API_KEY'
            )

        # ä»…å±•ç¤ºè„±æ•ä¿¡æ¯ï¼Œæ–¹ä¾¿æ’æŸ¥æ˜¯å¦è¯»å–åˆ°äº† Key
        print(f"ğŸ”‘ DashScope Key: {_mask_secret(api_key)}")

        # åŠ è½½è§’è‰²é…ç½®
        role_loader = RoleLoader()
        if role_config is None:
            role_config = role_loader.get_role("xuejie")

        # åˆå§‹åŒ–ä¸»ç¨‹åº
        self.llm_tts = LLMTTSTest(role_config=role_config)

        # æå‰æ ¡éªŒ LLM é…ç½®ï¼Œé¿å…è¿›å…¥è¯­éŸ³æµç¨‹åæ‰ 401
        llm_cfg = self.llm_tts.config.get("openai_compatible", {})
        if not llm_cfg.get("api_key"):
            raise ValueError(
                'æœªæ‰¾åˆ° LLM çš„ OPENAI_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½® OPENAI_API_KEY / OPENAI_BASE_URL / OPENAI_MODEL'
            )
        self.llm_tts.initialize_llm()

        # åˆå§‹åŒ– ASR
        self.asr = DashScopeASR(api_key=api_key, model=asr_model)
        print(f"ğŸ¤ ASR æ¨¡å‹: {asr_model}")

        # åˆå§‹åŒ– AEC å¤„ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.enable_aec = enable_aec
        self.aec_processor = None

        if enable_aec:
            # æ£€æŸ¥æ˜¯å¦æä¾›äº†è®¾å¤‡ç´¢å¼•
            if device_index is None:
                raise ValueError(
                    'å¯ç”¨ AEC æ—¶å¿…é¡»æä¾›èšåˆè®¾å¤‡ç´¢å¼•ï¼\n'
                    'è¯·å…ˆè¿è¡Œ: python -m src.main --list-devices\n'
                    'ç„¶åä½¿ç”¨: python -m src.main --voice --device-index <ç´¢å¼•>'
                )

            try:
                self.aec_processor = SimpleAEC(sample_rate=16000)
                print("ğŸ›ï¸ AECï¼ˆå›å£°æ¶ˆé™¤ï¼‰å·²å¯ç”¨ - ä½¿ç”¨èšåˆè®¾å¤‡ + BlackHole")
            except Exception as e:
                print(f"âš ï¸ AEC åˆå§‹åŒ–å¤±è´¥: {e}")
                print("   å°†ç»§ç»­è¿è¡Œä½†ä¸ä½¿ç”¨ AEC")
                self.enable_aec = False

        # åˆå§‹åŒ–éŸ³é¢‘è¾“å…¥ï¼ˆä¼ å…¥ AEC å¤„ç†å™¨å’Œèšåˆè®¾å¤‡é…ç½®ï¼‰
        # ä½¿ç”¨ WebRTC æ ‡å‡†å¸§å¤§å°ï¼š10ms = 160 samples @ 16kHz
        self.audio_input = AudioInput(
            sample_rate=16000,
            chunk_size=160,  # ä¿®æ”¹ä¸º 10msï¼ˆWebRTC æ ‡å‡†ï¼‰
            enable_aec=self.enable_aec,
            aec_processor=self.aec_processor,
            use_aggregate_device=enable_aec,  # å¯ç”¨ AEC å°±ä½¿ç”¨èšåˆè®¾å¤‡
            device_index=device_index
        )

        # åˆå§‹åŒ–æ‰“æ–­æ§åˆ¶å™¨
        self.interrupt_controller = InterruptController()

        # åˆå§‹åŒ– TTS å®¢æˆ·ç«¯ï¼ˆå…¨å±€å¤ç”¨ï¼Œåˆå§‹åŒ–æ—¶åˆ›å»ºä¸€æ¬¡ï¼‰
        volc_cfg = self.llm_tts.config.get("volcengine_seed2", {})
        volc_app_id = volc_cfg.get("app_id")
        volc_token = volc_cfg.get("access_token") or volc_cfg.get("api_key")

        if volc_app_id and volc_token:
            self.realtime_tts = VolcengineRealtimeTTS(
                app_id=volc_app_id,
                access_token=volc_token,
                voice="zh_female_cancan_mars_bigtts"
            )
            print('ğŸ”Š TTS: volcengine_seed2')
        else:
            # é»˜è®¤å›é€€åˆ° Qwen3 TTSï¼ˆä½¿ç”¨åŒä¸€ä¸ª DashScope Keyï¼‰
            self.realtime_tts = Qwen3RealtimeTTS(
                api_key=api_key,
                voice="Cherry",
                verbose=False
            )
            print('ğŸ”Š TTS: qwen3')

        # çŠ¶æ€
        self.is_listening = False
        self.is_tts_playing = False
        self.current_text = ""
        self.current_pipeline = None  # å½“å‰æ­£åœ¨è¿è¡Œçš„ pipeline
        self.current_player = None    # å½“å‰æ­£åœ¨æ’­æ”¾çš„ player
        self.current_tts_thread = None  # å½“å‰ TTS çº¿ç¨‹
        self.last_sentence_time = 0  # ä¸Šæ¬¡è§¦å‘å¯¹è¯çš„æ—¶é—´ï¼ˆé˜²æ­¢å¤ªå¿«é‡å¤è§¦å‘ï¼‰
        self.should_exit = False  # é€€å‡ºæ ‡å¿—

    def on_asr_text(self, text: str):
        """ASR ä¸­é—´ç»“æœå›è°ƒ"""
        # å¦‚æœ AI æ­£åœ¨è¯´è¯ï¼Œåªç”¨äºæ‰“æ–­æ£€æµ‹ï¼Œä¸æ˜¾ç¤º
        if self.is_tts_playing:
            self.interrupt_controller.on_asr_text(text, is_final=False)
            return

        self.current_text = text
        print(f'\rğŸ’¬ {text}', end='', flush=True)

    def on_asr_sentence(self, text: str):
        """ASR å®Œæ•´å¥å­å›è°ƒ"""
        # è¿‡æ»¤ç©ºæ–‡æœ¬å’Œå¤ªçŸ­çš„æ–‡æœ¬ï¼ˆé¿å…å™ªéŸ³è§¦å‘ï¼‰
        if not text or not text.strip() or len(text.strip()) < 2:
            return

        # æ£€æŸ¥é€€å‡ºå‘½ä»¤
        exit_keywords = ['é€€å‡º', 'å†è§', 'æ‹œæ‹œ', 'ç»“æŸå¯¹è¯', 'å…³é—­ç¨‹åº']
        if any(keyword in text.strip() for keyword in exit_keywords):
            print(f'\n\nğŸ‘‹ æ£€æµ‹åˆ°é€€å‡ºå‘½ä»¤: "{text}"')
            print('æ­£åœ¨é€€å‡ºç¨‹åº...')
            # è®¾ç½®é€€å‡ºæ ‡å¿—
            self.should_exit = True
            return

        # å¦‚æœ AI æ­£åœ¨è¯´è¯ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯çœŸå®æ‰“æ–­
        # ä¼˜åŒ–ï¼šé™ä½é˜ˆå€¼åˆ° 4 ä¸ªå­—ç¬¦ï¼Œæé«˜æ‰“æ–­å“åº”é€Ÿåº¦
        if self.is_tts_playing:
            text_length = len(text.strip())
            # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬ï¼ˆ< 3 ä¸ªå­—ç¬¦ï¼‰å’Œå¸¸è§å›å£°è¯
            if text_length < 3:
                print(f'\nâš ï¸ å¿½ç•¥çŸ­æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯å›å£°ï¼‰: "{text}" (é•¿åº¦: {text_length})')
                return

            # æ£€æŸ¥æ˜¯å¦æ˜¯å•ä¸ªå›å£°è¯ï¼ˆ1-2 ä¸ªå­—ç¬¦çš„å¸¸è§è¯ï¼‰
            echo_keywords = ['å—¯', 'å•Š', 'å“¦', 'å‘ƒ', 'å—¯å—¯', 'å•Šå•Š', 'å“¦å“¦']
            if text.strip() in echo_keywords:
                print(f'\nâš ï¸ å¿½ç•¥å›å£°å…³é”®è¯: "{text}"')
                return

            # 3 ä¸ªå­—ç¬¦ä»¥ä¸Šçš„æ–‡æœ¬è®¤ä¸ºæ˜¯çœŸå®æ‰“æ–­
            print(f'\nğŸ”” æ£€æµ‹åˆ°æ‰“æ–­: "{text}" (é•¿åº¦: {text_length})')

        # é˜²æ­¢è¯´è¯å¤ªå¿«æ—¶é‡å¤è§¦å‘ï¼ˆé—´éš”å°‘äº1ç§’çš„å¿½ç•¥ï¼‰
        current_time = time.time()
        if current_time - self.last_sentence_time < 1.0 and not self.is_tts_playing:
            return
        self.last_sentence_time = current_time

        # å¦‚æœæ­£åœ¨æ’­æ”¾ï¼Œç›´æ¥æ‰“æ–­å¹¶æ¸…ç©ºï¼ˆä¸ç­‰å¾…ï¼‰
        if self.is_tts_playing:
            self.on_interrupt()
            # ä¸ç­‰å¾…æ—§çº¿ç¨‹ï¼Œç›´æ¥ç»§ç»­ï¼ˆæ‰“æ–­å°±æ˜¯æ‰“æ–­ï¼Œä¸ç®¡æ—§çš„äº†ï¼‰

        print(f'\n\nğŸ‘¤ ä½ : {text}')
        self.current_text = ""

        # åœæ­¢ç›‘å¬
        self.is_listening = False

        # åœ¨æ–°çº¿ç¨‹ä¸­å¤„ç† LLM + TTSï¼Œé¿å…é˜»å¡ ASR
        tts_thread = threading.Thread(target=self._process_and_speak, args=(text,))
        tts_thread.daemon = True
        tts_thread.start()
        self.current_tts_thread = tts_thread

    def _process_and_speak(self, text: str):
        """åœ¨å•ç‹¬çº¿ç¨‹ä¸­å¤„ç† LLM + TTSï¼ˆä½¿ç”¨ä¸¤é˜¶æ®µç»“æ„åŒ–è¾“å‡ºè®°å¿†æ–¹æ¡ˆï¼‰"""
        try:
            # å‘é€ç»™ LLM å¹¶æ’­æ”¾å›å¤
            self.is_tts_playing = True

            # å¯åŠ¨æ‰“æ–­ç›‘å¬ï¼ˆåœ¨ TTS æ’­æ”¾æ—¶ï¼‰
            self.interrupt_controller.set_tts_speaking(True)

            # è·å–å¯¹è¯å†å²
            history = []
            for msg in self.llm_tts.voice_prompt.conversation_history:
                history.append(msg)

            # å¤ç”¨å…¨å±€ TTS å®¢æˆ·ç«¯ï¼ˆä¸å†æ¯æ¬¡åˆ›å»ºæ–°çš„ï¼‰
            streaming_player = PyAudioStreamPlayer(
                sample_rate=24000
            )

            # åˆ›å»ºå®æ—¶ç®¡é“
            pipeline = RealtimeStreamingPipeline()

            # ä¿å­˜å¼•ç”¨ä»¥ä¾¿æ‰“æ–­
            self.current_pipeline = pipeline
            self.current_player = streaming_player

            # ä½¿ç”¨ MemoryEnhancedChat çš„æµå¼è¾“å‡ºï¼ˆä¸¤é˜¶æ®µè®°å¿†ç®¡ç†ï¼‰
            def llm_stream_generator():
                """å°† MemoryEnhancedChat.chat_stream è½¬æ¢ä¸º pipeline éœ€è¦çš„æ ¼å¼"""
                for chunk in self.llm_tts.memory_chat.chat_stream(text, history):
                    yield chunk

            # è¿è¡Œå®æ—¶ç®¡é“ï¼ˆå¤ç”¨å…¨å±€ TTS å®¢æˆ·ç«¯ï¼‰
            result = pipeline.run(
                llm_stream=llm_stream_generator(),
                realtime_tts_client=self.realtime_tts,
                streaming_player=streaming_player,
                display_text=True
            )

            # ä¿å­˜å¯¹è¯å†å²ï¼ˆå³ä½¿è¢«æ‰“æ–­ä¹Ÿè¦ä¿å­˜ï¼Œå› ä¸º LLM å·²ç»ç”Ÿæˆäº†å›å¤ï¼‰
            if result and result.get("text"):
                self.llm_tts.voice_prompt.add_conversation('user', text)
                self.llm_tts.voice_prompt.add_conversation('assistant', result["text"])

        except Exception as e:
            print(f'\nâŒ é”™è¯¯: {e}')

        finally:
            # æ¸…é™¤å¼•ç”¨
            self.current_pipeline = None
            self.current_player = None

            # TTS æ’­æ”¾å®Œæˆ
            self.is_tts_playing = False
            self.interrupt_controller.set_tts_speaking(False)

            # é‡æ–°å¼€å§‹ç›‘å¬
            self.is_listening = True
            print('\n')

    def on_interrupt(self):
        """æ‰“æ–­å›è°ƒ"""
        # åœæ­¢ TTS ç®¡é“
        if self.current_pipeline:
            self.current_pipeline.stop()

        # ç«‹å³ç»“æŸæ—§ sessionï¼ˆç«å±±å¼•æ“é™åˆ¶åŒæ—¶åªèƒ½æœ‰1ä¸ªsessionï¼‰
        if hasattr(self, 'realtime_tts'):
            if hasattr(self.realtime_tts, 'finish'):
                self.realtime_tts.finish()
            if hasattr(self.realtime_tts, 'clear_queue'):
                self.realtime_tts.clear_queue()

        # åœæ­¢éŸ³é¢‘æ’­æ”¾
        if self.current_player:
            self.current_player.stop()

        self.is_tts_playing = False

    def start(self):
        """å¯åŠ¨è¯­éŸ³äº¤äº’"""
        print('\nğŸ™ï¸  å¯¼å¸ˆè¯„ä»·å­¦å§åŠ©æ‰‹ - è¯­éŸ³äº¤äº’æ¨¡å¼')
        if self.enable_aec:
            print('ğŸ›ï¸  AEC å›å£°æ¶ˆé™¤å·²å¯ç”¨')
        print('â”' * 50)
        print('ğŸ’¡ é€€å‡ºæ–¹å¼ï¼š')
        print('   1. è¯´"é€€å‡º"ã€"å†è§"ã€"æ‹œæ‹œ"ç­‰é€€å‡ºå‘½ä»¤')
        print('   2. æŒ‰ Ctrl+C å¼ºåˆ¶é€€å‡º')
        print('â”' * 50)
        print()

        try:
            # å¯åŠ¨éŸ³é¢‘è¾“å…¥ï¼ˆå…ˆå¯åŠ¨ï¼Œç¡®ä¿éº¦å…‹é£æ‰“å¼€ï¼‰
            self.audio_input.start(audio_callback=self.asr.send_audio)

            # å¯åŠ¨ ASRï¼ˆåŒæ­¥å¯åŠ¨ï¼Œç¡®ä¿é‰´æƒå¤±è´¥ç­‰é”™è¯¯èƒ½ç«‹åˆ»æš´éœ²ï¼‰
            self.asr.start(
                on_text=self.on_asr_text,
                on_sentence=self.on_asr_sentence
            )
            time.sleep(0.2)

            # å¯åŠ¨æ‰“æ–­æ§åˆ¶å™¨
            self.interrupt_controller.start_monitoring(
                interrupt_callback=self.on_interrupt
            )

            self.is_listening = True
            print('ğŸ¤ è¯·è¯´è¯...\n')

            # æŒç»­è¿è¡Œï¼Œæ£€æŸ¥é€€å‡ºæ ‡å¿—
            while not self.should_exit:
                time.sleep(0.1)

            # æ­£å¸¸é€€å‡º
            print('\nğŸ‘‹ å†è§!')

        except KeyboardInterrupt:
            print('\n\nğŸ‘‹ å†è§!')

        finally:
            # æ¸…ç†èµ„æº
            self.audio_input.stop()
            self.asr.stop()
            self.audio_input.close()
            self.interrupt_controller.stop_monitoring()

            # å…³é—­ AEC å¤„ç†å™¨
            if self.aec_processor:
                self.aec_processor.close()

            # æ–­å¼€ TTS è¿æ¥ï¼ˆå¤ç”¨çš„å…¨å±€å®¢æˆ·ç«¯ï¼‰
            if hasattr(self, 'realtime_tts'):
                if hasattr(self.realtime_tts, 'disconnect'):
                    self.realtime_tts.disconnect()

            # å…³é—­ Mem0 è¿æ¥ï¼Œç¡®ä¿æ•°æ®æŒä¹…åŒ–
            if self.llm_tts.mem0_manager:
                self.llm_tts.mem0_manager.close()


def load_env_file():
    """æ‰‹åŠ¨åŠ è½½ .env æ–‡ä»¶"""
    env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
    env_vars = {}
    if os.path.exists(env_path):
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    env_vars[key.strip()] = value.strip()
    return env_vars


def _mask_secret(value: str, show_last: int = 4) -> str:
    """Mask secrets for logs (avoid leaking full keys)."""
    if not value:
        return ""
    value = str(value).strip()
    if len(value) <= show_last:
        return "*" * len(value)
    return "*" * (len(value) - show_last) + value[-show_last:]


def check_asr_auth(asr_model: str = "paraformer-realtime-v2") -> int:
    """Quickly validate DashScope ASR auth without opening microphone."""
    env_vars = load_env_file()
    api_key = (
        env_vars.get('QWEN3_API_KEY') or os.getenv('QWEN3_API_KEY') or
        env_vars.get('DASHSCOPE_API_KEY') or os.getenv('DASHSCOPE_API_KEY')
    )

    if not api_key:
        print('âŒ æœªæ‰¾åˆ° DashScope API Keyï¼ˆQWEN3_API_KEY æˆ– DASHSCOPE_API_KEYï¼‰')
        return 2

    print(f"ğŸ”‘ DashScope Key: {_mask_secret(api_key)}")
    print(f"ğŸ¤ ASR æ¨¡å‹: {asr_model}")

    asr = DashScopeASR(api_key=api_key, model=asr_model)
    try:
        asr.start(on_text=lambda _: None, on_sentence=lambda _: None)
        # ç«‹å³åœæ­¢ï¼ŒåªéªŒè¯é‰´æƒ/è¿æ¥æ˜¯å¦æˆåŠŸ
        asr.stop()
        print('âœ… ASR é‰´æƒ/è¿æ¥æ­£å¸¸')
        return 0
    except Exception as e:
        print(f'âŒ ASR é‰´æƒ/è¿æ¥å¤±è´¥: {e}')
        return 1


def list_audio_devices():
    """åˆ—å‡ºæ‰€æœ‰éŸ³é¢‘è®¾å¤‡"""
    import pyaudio
    p = pyaudio.PyAudio()
    print("\nå¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡ï¼š")
    print("=" * 80)
    for i in range(p.get_device_count()):
        info = p.get_device_info_by_index(i)
        if info['maxInputChannels'] > 0:
            print(f"\nè®¾å¤‡ {i}: {info['name']}")
            print(f"  è¾“å…¥é€šé“æ•°: {info['maxInputChannels']}")
            print(f"  é‡‡æ ·ç‡: {int(info['defaultSampleRate'])} Hz")
            if 'Aggregate' in info['name'] or 'aggregate' in info['name'].lower():
                print("  â­ è¿™æ˜¯èšåˆè®¾å¤‡ï¼")
    print("=" * 80)
    print("\nä½¿ç”¨æ–¹æ³•ï¼š")
    print("  1. ç¦ç”¨ AECï¼ˆè€³æœºæ¨¡å¼ï¼‰ï¼špython -m src.main --voice --no-aec")
    print("  2. å¯ç”¨ AECï¼ˆå¤–æ”¾æ¨¡å¼ï¼‰ï¼špython -m src.main --voice --device-index <èšåˆè®¾å¤‡ç´¢å¼•>")
    p.terminate()


class LLMTTSTest:
    def __init__(self, config_path: str = "config/api_keys.json", role_config: dict = None):
        """
        åˆå§‹åŒ–æµ‹è¯•ç±»

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            role_config: è§’è‰²é…ç½®ï¼ˆä» role_loader åŠ è½½ï¼‰
        """
        # ä½¿ç”¨æ–°çš„é…ç½®åŠ è½½å™¨
        config_loader = ConfigLoader()
        self.config = config_loader.get_config()

        # é™é»˜æ‰“å°é…ç½®çŠ¶æ€
        # config_loader.print_status()

        self.llm_client = None
        self.output_dir = "data/audios"

        # åˆå§‹åŒ– Mem0 è®°å¿†ç®¡ç†å™¨
        mem0_config = self.config.get("mem0", {})
        self.mem0_manager = None
        if mem0_config.get("enable_mem0", False):
            self.mem0_manager = Mem0Manager(mem0_config)

        # ç”¨æˆ·IDï¼ˆä»é…ç½®è·å–ï¼‰
        self.user_id = mem0_config.get("user_id", "default_user")

        # è§’è‰²æè¿°ï¼ˆç”¨äºè®°å¿†å¢å¼ºå¯¹è¯ï¼‰
        self.role_description = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„è¯­éŸ³åŠ©æ‰‹ï¼Œè¯´è¯ç®€æ´è‡ªç„¶"
        if role_config:
            self.role_description = f"ä½ æ˜¯{role_config.get('name', 'åŠ©æ‰‹')}ï¼Œ{role_config.get('personality', 'å‹å¥½')}"

        # åˆå§‹åŒ–è®°å¿†å¢å¼ºå¯¹è¯ï¼ˆæ–°æ–¹æ¡ˆï¼šä¸¤é˜¶æ®µç»“æ„åŒ–è¾“å‡ºï¼‰
        llm_config = self.config.get("openai_compatible", {})
        self.memory_chat = MemoryEnhancedChat(
            api_key=llm_config.get("api_key"),
            base_url=llm_config.get("base_url"),
            model=llm_config.get("model", "qwen3-max"),
            mem0_manager=self.mem0_manager,
            user_id=self.user_id,
            role_description=self.role_description,
            verbose=False  # ç”Ÿäº§æ¨¡å¼å…³é—­è¯¦ç»†æ—¥å¿—
        )

        # åˆå§‹åŒ–è¯­éŸ³åŠ©æ‰‹ Prompt ç®¡ç†å™¨ï¼ˆä¿ç•™ç”¨äºå¯¹è¯å†å²ç®¡ç†ï¼‰
        self.voice_prompt = VoiceAssistantPrompt(role_config=role_config, mem0_manager=None)

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)

    def initialize_llm(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        llm_config = self.config.get("openai_compatible", {})
        self.llm_client = LLMClient(
            api_key=llm_config.get("api_key"),
            base_url=llm_config.get("base_url"),
            model=llm_config.get("model")
        )
        # print(f"âœ“ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ: {self.llm_client.get_model_info()}")  # é™é»˜åˆå§‹åŒ–

    def chat_and_speak_realtime(self, prompt: str, play_audio: bool = True):
        """
        çœŸæ­£çš„å®æ—¶å¯¹è¯ï¼ˆä¸¤é˜¶æ®µç»“æ„åŒ–è¾“å‡ºè®°å¿†æ–¹æ¡ˆï¼‰
        ç¬¬ä¸€é˜¶æ®µï¼šåˆ†ææ„å›¾ï¼Œæ£€ç´¢/å­˜å‚¨è®°å¿†
        ç¬¬äºŒé˜¶æ®µï¼šæµå¼ç”Ÿæˆå›å¤ â†’ å®æ—¶ TTS â†’ æµå¼æ’­æ”¾

        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            play_audio: æ˜¯å¦æ’­æ”¾éŸ³é¢‘
        """
        # è·å–å¯¹è¯å†å²
        history = []
        for msg in self.voice_prompt.conversation_history:
            history.append(msg)

        # åˆ›å»ºå®æ—¶ TTS å®¢æˆ·ç«¯
        config = self.config.get("qwen3_tts", {})
        realtime_tts = Qwen3RealtimeTTS(
            api_key=config.get("api_key"),
            voice="Cherry",
            verbose=False  # å…³é—­ TTS æ—¥å¿—
        )

        # åˆ›å»ºæµå¼æ’­æ”¾å™¨ - PyAudio
        streaming_player = PyAudioStreamPlayer(sample_rate=24000)

        # åˆ›å»ºå®æ—¶ç®¡é“
        pipeline = RealtimeStreamingPipeline()

        # ä½¿ç”¨ MemoryEnhancedChat çš„æµå¼è¾“å‡ºï¼ˆä¸¤é˜¶æ®µè®°å¿†ç®¡ç†ï¼‰
        def llm_stream_generator():
            """å°† MemoryEnhancedChat.chat_stream è½¬æ¢ä¸º pipeline éœ€è¦çš„æ ¼å¼"""
            for chunk in self.memory_chat.chat_stream(prompt, history):
                yield chunk

        # è¿è¡Œå®æ—¶ç®¡é“
        result = pipeline.run(
            llm_stream=llm_stream_generator(),
            realtime_tts_client=realtime_tts,
            streaming_player=streaming_player,
            display_text=True
        )

        # ä¿å­˜å¯¹è¯å†å²ï¼ˆçŸ­æœŸè®°å¿†ï¼‰
        self.voice_prompt.add_conversation('user', prompt)
        self.voice_prompt.add_conversation('assistant', result["text"])

        print()  # æ¢è¡Œ

        return {
            "prompt": prompt,
            "response": result["text"],
            "mode": "realtime",
            "metrics": result["metrics"]
        }

    def chat_and_speak_realtime_volcengine(self, prompt: str, play_audio: bool = True):
        """
        ç«å±±å¼•æ“å®æ—¶å¯¹è¯ï¼ˆä¸¤é˜¶æ®µç»“æ„åŒ–è¾“å‡ºè®°å¿†æ–¹æ¡ˆï¼‰
        ç¬¬ä¸€é˜¶æ®µï¼šåˆ†ææ„å›¾ï¼Œæ£€ç´¢/å­˜å‚¨è®°å¿†
        ç¬¬äºŒé˜¶æ®µï¼šæµå¼ç”Ÿæˆå›å¤ â†’ å®æ—¶ TTS â†’ æµå¼æ’­æ”¾

        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            play_audio: æ˜¯å¦æ’­æ”¾éŸ³é¢‘
        """
        # è·å–å¯¹è¯å†å²
        history = []
        for msg in self.voice_prompt.conversation_history:
            history.append(msg)

        # åˆ›å»ºå®æ—¶ TTS å®¢æˆ·ç«¯
        config = self.config.get("volcengine_seed2", {})
        realtime_tts = VolcengineRealtimeTTS(
            app_id=config.get("app_id"),
            access_token=config.get("access_token") or config.get("api_key"),
            voice="zh_female_cancan_mars_bigtts"
        )

        # åˆ›å»ºæµå¼æ’­æ”¾å™¨ - PyAudio
        streaming_player = PyAudioStreamPlayer(sample_rate=24000)

        # åˆ›å»ºå®æ—¶ç®¡é“
        pipeline = RealtimeStreamingPipeline()

        # ä½¿ç”¨ MemoryEnhancedChat çš„æµå¼è¾“å‡ºï¼ˆä¸¤é˜¶æ®µè®°å¿†ç®¡ç†ï¼‰
        def llm_stream_generator():
            """å°† MemoryEnhancedChat.chat_stream è½¬æ¢ä¸º pipeline éœ€è¦çš„æ ¼å¼"""
            for chunk in self.memory_chat.chat_stream(prompt, history):
                yield chunk

        # è¿è¡Œå®æ—¶ç®¡é“
        result = pipeline.run(
            llm_stream=llm_stream_generator(),
            realtime_tts_client=realtime_tts,
            streaming_player=streaming_player,
            display_text=True
        )

        # ä¿å­˜å¯¹è¯å†å²ï¼ˆçŸ­æœŸè®°å¿†ï¼‰
        self.voice_prompt.add_conversation('user', prompt)
        self.voice_prompt.add_conversation('assistant', result["text"])

        print()  # æ¢è¡Œ

        return {
            "prompt": prompt,
            "response": result["text"],
            "mode": "realtime_volcengine",
            "metrics": result["metrics"]
        }

    def interactive_mode(self, use_realtime: bool = True):
        """
        äº¤äº’å¼å¯¹è¯æ¨¡å¼

        Args:
            use_realtime: æ˜¯å¦ä½¿ç”¨å®æ—¶æ¨¡å¼ï¼ˆé»˜è®¤å¼€å¯ï¼‰
        """
        print("\n" + "="*60)
        print("ğŸ™ï¸  è¯­éŸ³åŠ©æ‰‹äº¤äº’æ¨¡å¼")
        print("="*60)
        print("\nå‘½ä»¤:")
        print("  /quit      - é€€å‡º")
        print("  /provider  - åˆ‡æ¢ TTS æä¾›å•†")
        print("  /role      - åˆ‡æ¢è§’è‰²")
        print("  /clear     - æ¸…ç©ºå¯¹è¯å†å²")
        print("  /memories  - æŸ¥çœ‹é•¿æœŸè®°å¿†")
        print("  /clearmem  - æ¸…é™¤é•¿æœŸè®°å¿†")
        print("  /user <ID> - åˆ‡æ¢ç”¨æˆ·")
        print()

        current_provider = "qwen3"

        provider_desc = {
            "qwen3": "é€šä¹‰åƒé—® TTS",
            "volcengine": "ç«å±±å¼•æ“ TTS"
        }

        print(f"å½“å‰ TTS: {provider_desc[current_provider]}")
        if self.mem0_manager and self.mem0_manager.enabled:
            print(f"è®°å¿†åŠŸèƒ½: âœ… å·²å¯ç”¨ (ç”¨æˆ·: {self.user_id})")
        else:
            print(f"è®°å¿†åŠŸèƒ½: âŒ æœªå¯ç”¨")
        print()

        while True:
            try:
                user_input = input("ğŸ’¬ ä½ : ").strip()

                if not user_input:
                    continue

                if user_input == "/quit":
                    print("\nğŸ‘‹ å†è§!")
                    break

                elif user_input == "/help":
                    print("\nğŸ“– å¯ç”¨å‘½ä»¤:")
                    print("  /quit      - é€€å‡ºç¨‹åº")
                    print("  /provider  - åˆ‡æ¢ TTS æä¾›å•† (qwen3/volcengine)")
                    print("  /role      - åˆ‡æ¢è§’è‰²")
                    print("  /clear     - æ¸…ç©ºå¯¹è¯å†å²")
                    print("  /history   - æŸ¥çœ‹å¯¹è¯å†å²")
                    print("  /memories  - æŸ¥çœ‹é•¿æœŸè®°å¿†")
                    print("  /clearmem  - æ¸…é™¤é•¿æœŸè®°å¿†")
                    print("  /user <ID> - åˆ‡æ¢ç”¨æˆ·")
                    print("  /info      - æŸ¥çœ‹å½“å‰é…ç½®")
                    print("  /help      - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
                    print()

                elif user_input == "/provider":
                    # å¾ªç¯åˆ‡æ¢ TTS æä¾›å•†
                    providers = ["qwen3", "volcengine"]
                    current_idx = providers.index(current_provider)
                    current_provider = providers[(current_idx + 1) % len(providers)]
                    print(f"âœ“ å·²åˆ‡æ¢åˆ°: {provider_desc[current_provider]}\n")

                elif user_input == "/clear":
                    self.voice_prompt.clear_history()
                    print("âœ“ å¯¹è¯å†å²å·²æ¸…ç©º\n")

                elif user_input.startswith("/role"):
                    parts = user_input.split(maxsplit=1)
                    if len(parts) > 1:
                        role = parts[1]
                        try:
                            self.voice_prompt.set_role(role)
                            role_info = self.voice_prompt.get_role_info()
                            print(f"âœ“ å·²åˆ‡æ¢åˆ°è§’è‰²: {role_info['name']}\n")
                        except ValueError as e:
                            print(f"âŒ {str(e)}\n")
                    else:
                        current_role = self.voice_prompt.get_role_info()
                        print(f"å½“å‰è§’è‰²: {current_role['name']}")
                        print("å¯é€‰è§’è‰²: default, casual, professional, companion\n")

                elif user_input == "/history":
                    summary = self.voice_prompt.get_conversation_summary()
                    print(f"\nå¯¹è¯å†å²:\n{summary}\n")

                elif user_input.startswith("/setname"):
                    parts = user_input.split(maxsplit=1)
                    if len(parts) > 1:
                        name = parts[1]
                        self.voice_prompt.set_user_info(name=name)
                        print(f"âœ“ ç”¨æˆ·åå·²è®¾ç½®ä¸º: {name}\n")
                    else:
                        print("âŒ è¯·æä¾›ç”¨æˆ·åï¼Œä¾‹å¦‚: /setname å°æ˜\n")

                elif user_input.startswith("/addknowledge"):
                    parts = user_input.split(maxsplit=1)
                    if len(parts) > 1:
                        knowledge = parts[1]
                        self.voice_prompt.add_knowledge(knowledge)
                        print(f"âœ“ å·²æ·»åŠ çŸ¥è¯†: {knowledge}\n")
                    else:
                        print("âŒ è¯·æä¾›çŸ¥è¯†å†…å®¹ï¼Œä¾‹å¦‚: /addknowledge ç”¨æˆ·å–œæ¬¢æ”€å²©\n")

                elif user_input == "/memories":
                    # æŸ¥çœ‹é•¿æœŸè®°å¿†
                    if self.mem0_manager and self.mem0_manager.enabled:
                        memories = self.mem0_manager.get_all_memories(self.user_id)
                        if memories:
                            print(f"\nğŸ“ é•¿æœŸè®°å¿† (ç”¨æˆ·: {self.user_id}):")
                            for i, mem in enumerate(memories, 1):
                                print(f"  {i}. {mem['memory']}")
                            print()
                        else:
                            print("æš‚æ— é•¿æœŸè®°å¿†\n")
                    else:
                        print("âŒ Mem0 æœªå¯ç”¨\n")

                elif user_input == "/clearmem":
                    # æ¸…é™¤é•¿æœŸè®°å¿†
                    if self.mem0_manager and self.mem0_manager.enabled:
                        self.mem0_manager.clear_memories(self.user_id)
                        print(f"âœ“ å·²æ¸…é™¤ç”¨æˆ· {self.user_id} çš„æ‰€æœ‰è®°å¿†\n")
                    else:
                        print("âŒ Mem0 æœªå¯ç”¨\n")

                elif user_input.startswith("/user"):
                    # åˆ‡æ¢ç”¨æˆ·
                    parts = user_input.split(maxsplit=1)
                    if len(parts) > 1:
                        self.user_id = parts[1]
                        # æ›´æ–° memory_chat çš„ user_id
                        self.memory_chat.user_id = self.user_id
                        print(f"âœ“ å·²åˆ‡æ¢åˆ°ç”¨æˆ·: {self.user_id}\n")
                    else:
                        print(f"å½“å‰ç”¨æˆ·: {self.user_id}\n")

                elif user_input == "/info":
                    role_info = self.voice_prompt.get_role_info()
                    print("\nå½“å‰é…ç½®:")
                    print(f"  TTS æä¾›å•†: {provider_desc[current_provider]}")
                    print(f"  è§’è‰²: {role_info['name']} ({role_info['personality']})")
                    print(f"  å¯¹è¯è½®æ•°: {len(self.voice_prompt.conversation_history) // 2}")
                    print(f"  çŸ¥è¯†åº“æ¡ç›®: {len(self.voice_prompt.knowledge_base)}")
                    if self.mem0_manager and self.mem0_manager.enabled:
                        mem_count = len(self.mem0_manager.get_all_memories(self.user_id))
                        print(f"  é•¿æœŸè®°å¿†: {mem_count} æ¡ (ç”¨æˆ·: {self.user_id})")
                    print()

                else:
                    # å®æ—¶å¯¹è¯
                    if current_provider == "qwen3":
                        self.chat_and_speak_realtime(user_input)
                    elif current_provider == "volcengine":
                        self.chat_and_speak_realtime_volcengine(user_input)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§!")
                # ç¡®ä¿æ­£ç¡®å…³é—­ Mem0 è¿æ¥
                if self.mem0_manager and self.mem0_manager.enabled:
                    self.mem0_manager.close()
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {str(e)}\n")

        # æ­£å¸¸é€€å‡ºæ—¶ä¹Ÿè¦å…³é—­è¿æ¥
        if self.mem0_manager and self.mem0_manager.enabled:
            self.mem0_manager.close()


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼"""
    parser = argparse.ArgumentParser(
        description='Personality TTS - ä¸ªæ€§åŒ–è¯­éŸ³åŠ©æ‰‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
è¿è¡Œæ¨¡å¼ï¼š
  æ–‡å­—å¯¹è¯æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼š
    python -m src.main
    python -m src.main --text

  è¯­éŸ³å¯¹è¯æ¨¡å¼ï¼š
    python -m src.main --voice --no-aec           # è€³æœºæ¨¡å¼ï¼ˆæ¨èï¼‰
    python -m src.main --voice --device-index <N> # AEC æ¨¡å¼

  å…¶ä»–å·¥å…·ï¼š
    python -m src.main --list-devices             # åˆ—å‡ºéŸ³é¢‘è®¾å¤‡
    python -m src.main --check-asr                # æ£€æŸ¥ ASR é‰´æƒ
        """
    )

    # è¿è¡Œæ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument('--text', action='store_true', help='æ–‡å­—å¯¹è¯æ¨¡å¼ï¼ˆä½ æ‰“å­—ï¼ŒAI è¯´è¯ï¼‰')
    mode_group.add_argument('--voice', action='store_true', help='è¯­éŸ³å¯¹è¯æ¨¡å¼ï¼ˆä½ è¯´è¯ï¼ŒAI è¯´è¯ï¼‰')

    # è¯­éŸ³æ¨¡å¼å‚æ•°
    parser.add_argument('--no-aec', action='store_true', help='ç¦ç”¨ AECï¼ˆå›å£°æ¶ˆé™¤ï¼‰ï¼Œä½¿ç”¨è€³æœºæ¨¡å¼')
    parser.add_argument('--device-index', type=int, help='èšåˆè®¾å¤‡ç´¢å¼•ï¼ˆå¯ç”¨ AEC æ—¶å¿…é¡»æä¾›ï¼‰')
    parser.add_argument('--asr-model', type=str, default='paraformer-realtime-v2',
                        choices=['paraformer-realtime-v2', 'fun-asr-realtime-2025-11-07'],
                        help='ASR æ¨¡å‹é€‰æ‹©ï¼ˆé»˜è®¤: paraformer-realtime-v2ï¼‰')

    # å·¥å…·å‘½ä»¤
    parser.add_argument('--check-asr', action='store_true', help='ä»…æ£€æŸ¥ ASR é‰´æƒ/è¿æ¥ï¼ˆä¸æ‰“å¼€éº¦å…‹é£ï¼‰')
    parser.add_argument('--list-devices', action='store_true', help='åˆ—å‡ºæ‰€æœ‰éŸ³é¢‘è®¾å¤‡')

    # è§’è‰²é€‰æ‹©
    parser.add_argument('--role', type=str, help='æŒ‡å®šè§’è‰²ï¼ˆå¦‚: natural, xuejie, funny ç­‰ï¼‰')

    args = parser.parse_args()

    # å·¥å…·å‘½ä»¤ä¼˜å…ˆå¤„ç†
    if args.list_devices:
        list_audio_devices()
        return

    if args.check_asr:
        raise SystemExit(check_asr_auth(asr_model=args.asr_model))

    # ç¡®å®šè¿è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ä¸ºæ–‡å­—å¯¹è¯æ¨¡å¼ï¼‰
    if args.voice:
        # è¯­éŸ³å¯¹è¯æ¨¡å¼
        try:
            # åŠ è½½è§’è‰²é…ç½®
            role_loader = RoleLoader()
            role_config = None
            if args.role:
                role_config = role_loader.get_role(args.role)
                if role_config:
                    print(f"\nâœ“ ä½¿ç”¨è§’è‰²: {role_config['name']}")
                    print(f"  ç‰¹ç‚¹: {role_config['personality']}")
                    print(f"  é£æ ¼: {role_config['style']}\n")
            else:
                # é»˜è®¤ä½¿ç”¨ xuejie è§’è‰²
                role_config = role_loader.get_role("xuejie")

            voice_mode = VoiceInteractiveMode(
                enable_aec=not args.no_aec,
                device_index=args.device_index,
                asr_model=args.asr_model,
                role_config=role_config
            )
            voice_mode.start()
        except Exception as e:
            print(f'é”™è¯¯: {e}')
            import traceback
            traceback.print_exc()
    else:
        # æ–‡å­—å¯¹è¯æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        print('\n' + '='*60)
        print('ğŸ’¬ æ–‡å­—å¯¹è¯æ¨¡å¼')
        print('='*60)
        print('\nâœ¨ æ”¯æŒæ™ºèƒ½è®°å¿†åŠŸèƒ½')
        print('   - LLM ä¼šè‡ªåŠ¨ä¿å­˜é‡è¦ä¿¡æ¯')
        print('   - ä½¿ç”¨ /memories æŸ¥çœ‹è®°å¿†')
        print('   - ä½¿ç”¨ /help æŸ¥çœ‹æ‰€æœ‰å‘½ä»¤\n')

        # åŠ è½½è§’è‰²
        role_loader = RoleLoader()

        # å¦‚æœæŒ‡å®šäº†è§’è‰²ï¼Œä½¿ç”¨æŒ‡å®šè§’è‰²ï¼›å¦åˆ™è®©ç”¨æˆ·é€‰æ‹©
        if args.role:
            role_config = role_loader.get_role(args.role)
            if role_config:
                print(f"âœ“ ä½¿ç”¨è§’è‰²: {role_config['name']}")
                print(f"  ç‰¹ç‚¹: {role_config['personality']}")
                print(f"  é£æ ¼: {role_config['style']}\n")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°è§’è‰² '{args.role}'ï¼Œä½¿ç”¨é»˜è®¤è§’è‰²")
                role_config = role_loader.get_role("natural")
        else:
            # è®©ç”¨æˆ·é€‰æ‹©è§’è‰²
            selected_role_id = role_loader.select_role_interactive()
            if selected_role_id:
                role_config = role_loader.get_role(selected_role_id)
                print(f"\nâœ“ ä½¿ç”¨è§’è‰²: {role_config['name']}")
                print(f"  ç‰¹ç‚¹: {role_config['personality']}")
                print(f"  é£æ ¼: {role_config['style']}\n")
            else:
                # é»˜è®¤ä½¿ç”¨ natural è§’è‰²
                role_config = role_loader.get_role("natural")

        print(f"æ­£åœ¨åˆå§‹åŒ–...")

        # åˆå§‹åŒ–
        test = LLMTTSTest(role_config=role_config)
        test.initialize_llm()

        print("âœ“ åˆå§‹åŒ–å®Œæˆ\n")

        # è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆä½¿ç”¨å®æ—¶æ¨¡å¼ï¼‰
        test.interactive_mode(use_realtime=True)


if __name__ == "__main__":
    main()
