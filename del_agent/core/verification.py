"""
Verification Loop - æ ¸éªŒå¾ªç¯æœºåˆ¶
å®ç° Agent Output â†’ Critic Check â†’ Pass/Retry çš„æ ¸å¿ƒé€»è¾‘
"""

from typing import Callable, Tuple, List, Any, Optional
from pydantic import BaseModel
import logging
import time
from datetime import datetime

# å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
try:
    from ..models.schemas import CriticFeedback
except (ImportError, ValueError):
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    from models.schemas import CriticFeedback

logger = logging.getLogger(__name__)


class VerificationLoop:
    """
    æ ¸éªŒå¾ªç¯å™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ‰§è¡Œç”Ÿæˆå™¨å‡½æ•°ï¼Œäº§ç”Ÿ Agent è¾“å‡º
    2. è°ƒç”¨åˆ¤åˆ«å™¨å‡½æ•°ï¼Œè¯„ä¼°è¾“å‡ºè´¨é‡
    3. æ ¹æ®åˆ¤åˆ«ç»“æœå†³å®šæ˜¯å¦é‡è¯•
    4. è®°å½•å®Œæ•´çš„åé¦ˆå†å²
    
    è®¾è®¡æ¨¡å¼ï¼šStrategy Patternï¼ˆç­–ç•¥æ¨¡å¼ï¼‰
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        loop = VerificationLoop(max_retries=3, strictness_level=0.7)
        result, history = loop.execute(
            generator_func=lambda: agent.process(input_data),
            critic_func=lambda output, ctx: critic.evaluate(output, ctx),
            context=original_input
        )
    """
    
    def __init__(
        self,
        max_retries: int = 3,
        strictness_level: float = 0.7,
        enable_logging: bool = True
    ):
        """
        åˆå§‹åŒ–æ ¸éªŒå¾ªç¯å™¨
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆä¸åŒ…æ‹¬é¦–æ¬¡å°è¯•ï¼‰
            strictness_level: ä¸¥æ ¼åº¦ç­‰çº§ï¼Œ0.0-1.0
                - 0.5: å®½æ¾ï¼ˆå…è®¸è½»å¾®åå·®ï¼‰
                - 0.7: æ ‡å‡†ï¼ˆæ­£å¸¸è´¨é‡è¦æ±‚ï¼‰
                - 0.9: ä¸¥æ ¼ï¼ˆè¦æ±‚è¿‘ä¹å®Œç¾ï¼‰
            enable_logging: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—
        """
        if max_retries < 0:
            raise ValueError("max_retries must be non-negative")
        if not 0 <= strictness_level <= 1:
            raise ValueError("strictness_level must be between 0 and 1")
        
        self.max_retries = max_retries
        self.strictness_level = strictness_level
        self.enable_logging = enable_logging
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_executions = 0
        self.successful_executions = 0
        self.failed_executions = 0
        
        if enable_logging:
            logger.info(
                f"VerificationLoop initialized: "
                f"max_retries={max_retries}, strictness={strictness_level}"
            )
    
    def execute(
        self,
        generator_func: Callable[[], BaseModel],
        critic_func: Callable[[BaseModel, Any], CriticFeedback],
        context: Any = None
    ) -> Tuple[BaseModel, List[CriticFeedback]]:
        """
        æ‰§è¡Œæ ¸éªŒå¾ªç¯
        
        Args:
            generator_func: ç”Ÿæˆå™¨å‡½æ•°ï¼Œæ— å‚æ•°ï¼Œè¿”å› Pydantic æ¨¡å‹å®ä¾‹
                ç¤ºä¾‹: lambda: agent.process(input_data)
            critic_func: åˆ¤åˆ«å™¨å‡½æ•°ï¼Œæ¥æ”¶ (output, context)ï¼Œè¿”å› CriticFeedback
                ç¤ºä¾‹: lambda output, ctx: critic.evaluate(output, ctx)
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆé€šå¸¸æ˜¯åŸå§‹è¾“å…¥ï¼‰ï¼Œä¼ é€’ç»™åˆ¤åˆ«å™¨
        
        Returns:
            Tuple[BaseModel, List[CriticFeedback]]:
                - æœ€ç»ˆè¾“å‡ºç»“æœï¼ˆPydantic æ¨¡å‹å®ä¾‹ï¼‰
                - æ‰€æœ‰åé¦ˆå†å²åˆ—è¡¨
        
        Raises:
            Exception: å¦‚æœç”Ÿæˆå™¨æˆ–åˆ¤åˆ«å™¨æŠ›å‡ºå¼‚å¸¸
        """
        self.total_executions += 1
        feedback_history: List[CriticFeedback] = []
        output: Optional[BaseModel] = None
        
        start_time = time.time()
        total_attempts = self.max_retries + 1  # åŒ…æ‹¬é¦–æ¬¡å°è¯•
        
        if self.enable_logging:
            logger.info(f"Starting verification loop (max attempts: {total_attempts})")
        
        for attempt in range(total_attempts):
            try:
                # Step 1: ç”Ÿæˆè¾“å‡º
                if self.enable_logging:
                    logger.debug(f"Attempt {attempt + 1}/{total_attempts}: Generating output...")
                
                generation_start = time.time()
                output = generator_func()
                generation_time = time.time() - generation_start
                
                if self.enable_logging:
                    logger.debug(f"Output generated in {generation_time:.2f}s")
                
                # Step 2: åˆ¤åˆ«æ£€æŸ¥
                if self.enable_logging:
                    logger.debug(f"Evaluating output with critic...")
                
                critic_start = time.time()
                feedback = critic_func(output, context)
                critic_time = time.time() - critic_start
                
                # ç¡®ä¿ feedback æ˜¯ CriticFeedback å®ä¾‹
                if not isinstance(feedback, CriticFeedback):
                    logger.error(f"Invalid feedback type: {type(feedback)}")
                    raise TypeError(
                        f"critic_func must return CriticFeedback, got {type(feedback)}"
                    )
                
                feedback_history.append(feedback)
                
                if self.enable_logging:
                    logger.debug(
                        f"Critic evaluation completed in {critic_time:.2f}s: "
                        f"approved={feedback.is_approved}, "
                        f"confidence={feedback.confidence_score:.2f}"
                    )
                
                # Step 3: åˆ¤æ–­æ˜¯å¦é€šè¿‡
                if feedback.is_approved:
                    self.successful_executions += 1
                    elapsed_time = time.time() - start_time
                    
                    if self.enable_logging:
                        logger.info(
                            f"âœ… Verification passed on attempt {attempt + 1}/{total_attempts} "
                            f"(total time: {elapsed_time:.2f}s)"
                        )
                    
                    return output, feedback_history
                
                # æœªé€šè¿‡ï¼Œè®°å½•åŸå› 
                if self.enable_logging:
                    logger.warning(
                        f"âŒ Attempt {attempt + 1}/{total_attempts} failed: "
                        f"{feedback.reasoning}"
                    )
                    if feedback.suggestion:
                        logger.info(f"ğŸ’¡ Suggestion: {feedback.suggestion}")
                
            except Exception as e:
                error_msg = f"Error in verification loop (attempt {attempt + 1}): {str(e)}"
                logger.error(error_msg)
                
                # åˆ›å»ºä¸€ä¸ªè¡¨ç¤ºé”™è¯¯çš„åé¦ˆ
                error_feedback = CriticFeedback(
                    is_approved=False,
                    reasoning=f"Exception occurred: {str(e)}",
                    suggestion="Check the generator or critic function for errors",
                    confidence_score=0.0
                )
                feedback_history.append(error_feedback)
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if attempt == total_attempts - 1:
                    raise
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        self.failed_executions += 1
        elapsed_time = time.time() - start_time
        
        if self.enable_logging:
            logger.error(
                f"âŒ Verification failed after {total_attempts} attempts "
                f"(total time: {elapsed_time:.2f}s)"
            )
        
        # è¿”å›æœ€åä¸€æ¬¡çš„è¾“å‡ºï¼ˆå³ä½¿æœªé€šè¿‡éªŒè¯ï¼‰
        return output, feedback_history
    
    def get_statistics(self) -> dict:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«æ‰§è¡Œç»Ÿè®¡çš„å­—å…¸
        """
        success_rate = (
            self.successful_executions / self.total_executions
            if self.total_executions > 0
            else 0.0
        )
        
        return {
            "total_executions": self.total_executions,
            "successful_executions": self.successful_executions,
            "failed_executions": self.failed_executions,
            "success_rate": success_rate
        }
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.total_executions = 0
        self.successful_executions = 0
        self.failed_executions = 0
        
        if self.enable_logging:
            logger.info("Statistics reset")


class AdaptiveVerificationLoop(VerificationLoop):
    """
    è‡ªé€‚åº”æ ¸éªŒå¾ªç¯å™¨
    
    å¢å¼ºåŠŸèƒ½ï¼š
    1. æ ¹æ®å†å²é€šè¿‡ç‡åŠ¨æ€è°ƒæ•´ max_retries
    2. æ”¯æŒæ—©åœç­–ç•¥ï¼ˆè¿ç»­å¤šæ¬¡å¤±è´¥åæå‰ç»ˆæ­¢ï¼‰
    3. å­¦ä¹ æœ€ä¼˜çš„ strictness_level
    
    é€‚ç”¨åœºæ™¯ï¼šé•¿æœŸè¿è¡Œçš„ç³»ç»Ÿï¼Œéœ€è¦è‡ªåŠ¨ä¼˜åŒ–æ€§èƒ½
    """
    
    def __init__(
        self,
        max_retries: int = 3,
        strictness_level: float = 0.7,
        enable_logging: bool = True,
        adaptation_window: int = 100,
        min_retries: int = 1,
        max_max_retries: int = 10
    ):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”æ ¸éªŒå¾ªç¯å™¨
        
        Args:
            max_retries: åˆå§‹æœ€å¤§é‡è¯•æ¬¡æ•°
            strictness_level: åˆå§‹ä¸¥æ ¼åº¦ç­‰çº§
            enable_logging: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—
            adaptation_window: è‡ªé€‚åº”çª—å£å¤§å°ï¼ˆå¤šå°‘æ¬¡æ‰§è¡Œåé‡æ–°è¯„ä¼°ï¼‰
            min_retries: æœ€å°é‡è¯•æ¬¡æ•°ï¼ˆè‡ªé€‚åº”ä¸‹é™ï¼‰
            max_max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°çš„ä¸Šé™ï¼ˆè‡ªé€‚åº”ä¸Šé™ï¼‰
        """
        super().__init__(max_retries, strictness_level, enable_logging)
        
        self.adaptation_window = adaptation_window
        self.min_retries = min_retries
        self.max_max_retries = max_max_retries
        
        # å†å²è®°å½•ï¼ˆç”¨äºè‡ªé€‚åº”ï¼‰
        self.recent_results: List[bool] = []  # True=æˆåŠŸ, False=å¤±è´¥
        self.recent_attempts: List[int] = []  # è®°å½•æ¯æ¬¡æˆåŠŸæ‰€éœ€çš„å°è¯•æ¬¡æ•°
    
    def execute(
        self,
        generator_func: Callable[[], BaseModel],
        critic_func: Callable[[BaseModel, Any], CriticFeedback],
        context: Any = None
    ) -> Tuple[BaseModel, List[CriticFeedback]]:
        """
        æ‰§è¡Œè‡ªé€‚åº”æ ¸éªŒå¾ªç¯
        """
        output, feedback_history = super().execute(generator_func, critic_func, context)
        
        # è®°å½•ç»“æœ
        is_successful = feedback_history[-1].is_approved if feedback_history else False
        self.recent_results.append(is_successful)
        self.recent_attempts.append(len(feedback_history))
        
        # è‡ªé€‚åº”è°ƒæ•´
        if len(self.recent_results) >= self.adaptation_window:
            self._adapt_parameters()
        
        return output, feedback_history
    
    def _adapt_parameters(self):
        """
        æ ¹æ®å†å²æ•°æ®è‡ªé€‚åº”è°ƒæ•´å‚æ•°
        """
        if not self.recent_results:
            return
        
        # è®¡ç®—æœ€è¿‘çš„æˆåŠŸç‡
        recent_success_rate = sum(self.recent_results) / len(self.recent_results)
        
        # è®¡ç®—å¹³å‡å°è¯•æ¬¡æ•°ï¼ˆä»…ç»Ÿè®¡æˆåŠŸçš„ï¼‰
        successful_attempts = [
            attempts for success, attempts in zip(self.recent_results, self.recent_attempts)
            if success
        ]
        avg_attempts = (
            sum(successful_attempts) / len(successful_attempts)
            if successful_attempts else self.max_retries
        )
        
        # è°ƒæ•´ç­–ç•¥
        old_max_retries = self.max_retries
        
        if recent_success_rate > 0.9:
            # æˆåŠŸç‡å¾ˆé«˜ï¼Œå¯ä»¥é™ä½é‡è¯•æ¬¡æ•°ä»¥æé«˜æ•ˆç‡
            self.max_retries = max(self.min_retries, self.max_retries - 1)
        elif recent_success_rate < 0.7:
            # æˆåŠŸç‡è¾ƒä½ï¼Œå¢åŠ é‡è¯•æ¬¡æ•°
            self.max_retries = min(self.max_max_retries, self.max_retries + 1)
        
        if old_max_retries != self.max_retries and self.enable_logging:
            logger.info(
                f"ğŸ”§ Adaptive adjustment: max_retries {old_max_retries} â†’ {self.max_retries} "
                f"(success_rate={recent_success_rate:.2%}, avg_attempts={avg_attempts:.1f})"
            )
        
        # æ¸…ç©ºå†å²ï¼ˆå¼€å§‹æ–°çš„çª—å£ï¼‰
        self.recent_results.clear()
        self.recent_attempts.clear()
