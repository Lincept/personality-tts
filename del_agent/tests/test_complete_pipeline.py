"""
å®Œæ•´çš„æ•°æ®å·¥å‚æµæ°´çº¿æµ‹è¯• - å±•ç¤º del_agent æ ¸å¿ƒåŠŸèƒ½
è¿™ä¸ªæµ‹è¯•æ¨¡æ‹ŸçœŸå®ç”¨æˆ·ä½¿ç”¨åœºæ™¯ï¼Œå±•ç¤ºä»åŸå§‹è¯„è®ºåˆ°ç»“æ„åŒ–çŸ¥è¯†èŠ‚ç‚¹çš„å®Œæ•´å¤„ç†æµç¨‹
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv

from core.llm_adapter import OpenAICompatibleProvider
from backend.factory import DataFactoryPipeline
from models.schemas import RawReview

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def print_separator(title: str = "", width: int = 80):
    """æ‰“å°åˆ†éš”çº¿"""
    if title:
        print(f"\n{'=' * width}")
        print(f" {title}")
        print(f"{'=' * width}")
    else:
        print("-" * width)


def format_json(obj, indent: int = 2) -> str:
    """æ ¼å¼åŒ–å¯¹è±¡ä¸ºJSONå­—ç¬¦ä¸²"""
    if hasattr(obj, 'model_dump'):
        return json.dumps(obj.model_dump(), ensure_ascii=False, indent=indent)
    return json.dumps(obj, ensure_ascii=False, indent=indent)


def main():
    print_separator("del_agent æ•°æ®å·¥å‚å®Œæ•´æµæ°´çº¿æµ‹è¯•", 100)
    
    # 1. é…ç½® LLM æä¾›è€…
    print("\nğŸ“‹ æ­¥éª¤ 1: é…ç½® LLM æä¾›è€…")
    print_separator("", 100)
    
    api_key = os.getenv("ARK_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° ARK_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    print(f"âœ“ ä½¿ç”¨è±†åŒ…æ¨¡å‹")
    print(f"âœ“ API Key: {api_key[:15]}...")
    
    llm_provider = OpenAICompatibleProvider(
        model_name="doubao-seed-1-6-251015",
        api_key=api_key,
        base_url="https://ark.cn-beijing.volces.com/api/v3",
        timeout=60
    )
    print("âœ“ LLM æä¾›è€…åˆ›å»ºæˆåŠŸ")
    
    # 2. åˆå§‹åŒ–æ•°æ®å·¥å‚æµæ°´çº¿
    print_separator("æ­¥éª¤ 2: åˆå§‹åŒ–æ•°æ®å·¥å‚æµæ°´çº¿", 100)
    
    pipeline = DataFactoryPipeline(
        llm_provider=llm_provider,
        enable_verification=False,  # å…ˆä¸å¯ç”¨æ ¸éªŒï¼Œæé«˜é€Ÿåº¦
        slang_dict_storage="json",
        slang_dict_path="./slang_dictionary.json"
    )
    print("âœ“ æ•°æ®å·¥å‚æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    print("  åŒ…å«æ™ºèƒ½ä½“: RawCommentCleaner â†’ SlangDecoder â†’ Weigher â†’ Compressor")
    
    # 3. å‡†å¤‡æµ‹è¯•æ•°æ®
    print_separator("æ­¥éª¤ 3: å‡†å¤‡æµ‹è¯•æ•°æ®", 100)
    
    test_reviews = [
        {
            "content": "è¿™è€æ¿ç®€ç›´æ˜¯'å­¦æœ¯å¦²å·±'ï¼Œå¤ªä¼šç”»é¥¼äº†ï¼ç»è´¹å€’æ˜¯å¤šï¼Œä½†æ´¥è´´ä¸å‘ç»™æˆ‘ä»¬ã€‚",
            "metadata": {
                "platform": "çŸ¥ä¹",
                "author_id": "student_001",
                "author_role": "åšå£«ç”Ÿ",
                "post_time": "2025-12-01"
            }
        },
        {
            "content": "å¯¼å¸ˆäººå¾ˆniceï¼Œç§‘ç ”ç»è´¹ç»™çš„å¾ˆè¶³ï¼Œå®éªŒè®¾å¤‡ä¹Ÿå¾ˆå…ˆè¿›ï¼Œå°±æ˜¯ç»å¸¸'æ”¾ç¾Š'ï¼Œä¸æ€ä¹ˆæŒ‡å¯¼ã€‚",
            "metadata": {
                "platform": "å°æœ¨è™«",
                "author_id": "student_002",
                "author_role": "ç¡•å£«ç”Ÿ",
                "post_time": "2025-11-20"
            }
        },
        {
            "content": "å¯¼å¸ˆå­¦æœ¯æ°´å¹³å¾ˆé«˜ï¼Œåœ¨é¢†åŸŸå†…å¾ˆæœ‰å½±å“åŠ›ï¼Œä½†æ˜¯å¯¹å­¦ç”Ÿè¦æ±‚å¤ªä¸¥æ ¼äº†ï¼Œå‹åŠ›å±±å¤§ã€‚",
            "metadata": {
                "platform": "è±†ç“£",
                "author_id": "student_003",
                "author_role": "åšå£«ç”Ÿ",
                "post_time": "2025-10-15"
            }
        }
    ]
    
    print(f"âœ“ å‡†å¤‡äº† {len(test_reviews)} æ¡æµ‹è¯•è¯„è®º")
    for i, review in enumerate(test_reviews, 1):
        print(f"\n  è¯„è®º {i}:")
        print(f"    å†…å®¹: {review['content'][:50]}...")
        print(f"    æ¥æº: {review['metadata']['platform']}")
        print(f"    ä½œè€…è§’è‰²: {review['metadata']['author_role']}")
    
    # 4. æ‰¹é‡å¤„ç†è¯„è®º
    print_separator("æ­¥éª¤ 4: æ‰¹é‡å¤„ç†è¯„è®º", 100)
    print("å¼€å§‹å¤„ç†æµæ°´çº¿...\n")
    
    raw_reviews = []
    for review_data in test_reviews:
        raw_review = RawReview(
            content=review_data["content"],
            source_metadata=review_data["metadata"]
        )
        raw_reviews.append(raw_review)
    
    # ä½¿ç”¨æµæ°´çº¿æ‰¹é‡å¤„ç†
    knowledge_nodes = pipeline.process_batch(raw_reviews)
    
    # 5. å±•ç¤ºå¤„ç†ç»“æœ
    print_separator("æ­¥éª¤ 5: å¤„ç†ç»“æœå±•ç¤º", 100)
    
    for i, node in enumerate(knowledge_nodes, 1):
        print(f"\n{'â–¼' * 50}")
        print(f"è¯„è®º {i} å¤„ç†ç»“æœ:")
        print(f"{'â–¼' * 50}")
        
        print(f"âœ“ å¤„ç†æˆåŠŸ")
        
        # å±•ç¤ºçŸ¥è¯†èŠ‚ç‚¹
        print(f"\nã€çŸ¥è¯†èŠ‚ç‚¹ã€‘")
        print(f"  å¯¼å¸ˆID: {node.mentor_id}")
        print(f"  è¯„ä»·ç»´åº¦: {node.dimension}")
        print(f"  ç»¼åˆæƒé‡: {node.weight_score:.3f}")
        
        # äº‹å®å†…å®¹
        print(f"\n  äº‹å®å†…å®¹:")
        print(f"    {node.fact_content}")
        
        # åŸæ–‡ç‰¹è‰²
        if node.original_nuance:
            print(f"\n  åŸæ–‡ç‰¹è‰²/é»‘è¯:")
            print(f"    {node.original_nuance}")
        
        # æ ‡ç­¾
        if node.tags:
            print(f"\n  æ ‡ç­¾: {', '.join(node.tags)}")
        
        # å…ƒæ•°æ®
        print(f"\n  æ›´æ–°æ—¶é—´: {node.last_updated}")
    
    # 6. ç»Ÿè®¡ä¿¡æ¯
    print_separator("æ­¥éª¤ 6: ç»Ÿè®¡ä¿¡æ¯", 100)
    
    stats = pipeline.get_statistics()
    print(f"\nå¤„ç†ç»Ÿè®¡:")
    print(f"  æ€»å¤„ç†æ•°: {stats['total_processed']}")
    print(f"  æˆåŠŸæ•°: {stats['successful']}")
    print(f"  å¤±è´¥æ•°: {stats['failed']}")
    print(f"  æˆåŠŸç‡: {stats.get('success_rate', 0) * 100:.1f}%")
    
    # 7. ä¿å­˜ç»“æœ
    print_separator("æ­¥éª¤ 7: ä¿å­˜ç»“æœ", 100)
    
    output_dir = Path(__file__).parent.parent / "output"
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"test_results_{timestamp}.json"
    
    # å°†ç»“æœè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    output_data = {
        "test_time": timestamp,
        "config": {
            "enable_verification": False,
            "model": "doubao-seed-1-6-251015"
        },
        "results": []
    }
    
    for i, node in enumerate(knowledge_nodes):
        result_data = {
            "review_index": i + 1,
            "original_content": test_reviews[i]["content"],
            "success": True,
            "knowledge_node": {
                **node.model_dump(),
                # è½¬æ¢ datetime ä¸ºå­—ç¬¦ä¸²
                "last_updated": node.last_updated.isoformat() if hasattr(node.last_updated, 'isoformat') else str(node.last_updated)
            }
        }
        
        output_data["results"].append(result_data)
    
    output_data["statistics"] = stats
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # 8. æµ‹è¯•æ ¸éªŒå¾ªç¯ï¼ˆå¯é€‰ï¼‰
    print_separator("æ­¥éª¤ 8: æµ‹è¯•æ ¸éªŒå¾ªç¯åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰", 100)
    
    print("\næ˜¯å¦è¦æµ‹è¯•æ ¸éªŒå¾ªç¯åŠŸèƒ½? (ä¼šå¢åŠ å¤„ç†æ—¶é—´)")
    print("æ ¸éªŒå¾ªç¯ä¼šä½¿ç”¨ CriticAgent å¯¹ç»“æœè¿›è¡Œè´¨é‡è¯„ä¼°å’Œæ”¹è¿›")
    print("æœ¬æ¬¡æµ‹è¯•è·³è¿‡æ ¸éªŒå¾ªç¯ä»¥åŠ å¿«é€Ÿåº¦")
    print("å¦‚éœ€æµ‹è¯•ï¼Œè¯·è®¾ç½® enable_verification=True")
    
    print_separator("æµ‹è¯•å®Œæˆ!", 100)
    print("\nâœ¨ del_agent æ•°æ®å·¥å‚æ ¸å¿ƒåŠŸèƒ½æ¼”ç¤ºå®Œæ¯•")
    print("âœ¨ ä»åŸå§‹è¯„è®º â†’ æ¸…æ´— â†’ é»‘è¯è§£ç  â†’ æƒé‡åˆ†æ â†’ ç»“æ„åŒ–å‹ç¼© â†’ çŸ¥è¯†èŠ‚ç‚¹")
    print(f"âœ¨ æŸ¥çœ‹è¯¦ç»†ç»“æœ: {output_file}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
