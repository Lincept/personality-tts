"""
AI Data Factory - è±†åŒ…æ¨¡å‹çœŸå®æµ‹è¯•
ä½¿ç”¨ç«å±±å¼•æ“ARK APIæµ‹è¯•å®Œæ•´çš„æ•°æ®å·¥å‚æµæ°´çº¿
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from dotenv import load_dotenv
from core.llm_adapter import OpenAICompatibleProvider
from backend.factory import DataFactoryPipeline
from models.schemas import RawReview

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def print_separator(title: str = ""):
    """æ‰“å°åˆ†éš”çº¿"""
    if title:
        print(f"\n{'='*80}")
        print(f"  {title}")
        print(f"{'='*80}\n")
    else:
        print(f"\n{'-'*80}\n")


def load_ark_api():
    """åŠ è½½ç«å±±å¼•æ“ARK APIé…ç½®"""
    load_dotenv()
    
    api_key = os.getenv("ARK_API_KEY")
    if not api_key:
        raise ValueError("ARK_API_KEY not found in .env file")
    
    logger.info(f"âœ“ ARK API Key loaded: {api_key[:8]}...")
    return api_key


def create_test_reviews():
    """åˆ›å»ºæµ‹è¯•è¯„è®ºæ•°æ®"""
    return [
        RawReview(
            review_id="test_001",
            content="è¿™è€æ¿ç®€ç›´æ˜¯'å­¦æœ¯å¦²å·±'ï¼Œå¤ªä¼šç”»é¥¼äº†ï¼ç»è´¹å€’æ˜¯å¤šï¼Œä½†ä¸å‘ç»™æˆ‘ä»¬ã€‚å®éªŒå®¤è®¾å¤‡è€æ—§ï¼Œç”³è¯·æ–°è®¾å¤‡æ€»æ˜¯æ‹–å»¶ã€‚",
            source="test",
            timestamp=datetime.now()
        ),
        RawReview(
            review_id="test_002",
            content="å®éªŒå®¤æ°›å›´è¿˜è¡Œï¼Œå¸ˆå…„å¸ˆå§éƒ½æŒºå‹å¥½çš„ï¼Œå¯¼å¸ˆäººä¹Ÿä¸é”™ã€‚å°±æ˜¯é¡¹ç›®æ–¹å‘æœ‰ç‚¹å†·é—¨ï¼Œæ‹…å¿ƒæ¯•ä¸šåä¸å¥½æ‰¾å·¥ä½œã€‚",
            source="test",
            timestamp=datetime.now()
        ),
        RawReview(
            review_id="test_003",
            content="å¯¼å¸ˆæ˜¯'å­¦é˜€'ï¼Œå¤©å¤©è®©æˆ‘ä»¬ç»™ä»–å¹²ç§æ´»ï¼Œè¿˜ä¸ç»™é’±ã€‚è®ºæ–‡ç½²åä¹Ÿä¸å…¬å¹³ï¼Œæ˜æ˜æ˜¯æˆ‘åšçš„å·¥ä½œï¼Œä»–è¦æŒ‚ç¬¬ä¸€ä½œè€…ã€‚å‹æ¦¨å­¦ç”Ÿå¤ªä¸¥é‡äº†ï¼",
            source="test",
            timestamp=datetime.now()
        ),
        RawReview(
            review_id="test_004",
            content="ç ”ç©¶æ–¹å‘æŒºå‰æ²¿çš„ï¼Œå®éªŒå®¤ä¹Ÿæœ‰å……è¶³çš„ç»è´¹æ”¯æŒã€‚å¯¼å¸ˆæŒ‡å¯¼å¾ˆè®¤çœŸï¼Œå®šæœŸå¼€ç»„ä¼šè®¨è®ºè¿›å±•ã€‚å°±æ˜¯å‘è®ºæ–‡å‹åŠ›æ¯”è¾ƒå¤§ã€‚",
            source="test",
            timestamp=datetime.now()
        ),
        RawReview(
            review_id="test_005",
            content="è¿™ä¸ªç»„ç®€ç›´æ˜¯'è¡€æ±—å·¥å‚'ï¼æ¯å¤©å·¥ä½œ12å°æ—¶ä»¥ä¸Šï¼Œå‘¨æœ«ä¹Ÿä¸è®©ä¼‘æ¯ã€‚è€æ¿è¿˜å–œæ¬¢PUAï¼ŒåŠ¨ä¸åŠ¨å°±è¯´æˆ‘ä»¬ä¸åŠªåŠ›ã€‚å—ä¸äº†äº†ï¼",
            source="test",
            timestamp=datetime.now()
        ),
    ]


def test_basic_llm():
    """æµ‹è¯•1ï¼šåŸºç¡€LLMè¿æ¥"""
    print_separator("æµ‹è¯•1ï¼šåŸºç¡€LLMè¿æ¥æµ‹è¯•")
    
    try:
        api_key = load_ark_api()
        
        # åˆ›å»ºLLMæä¾›è€…ï¼ˆä½¿ç”¨ç«å±±å¼•æ“ARK APIï¼‰
        llm_provider = OpenAICompatibleProvider(
            model_name="doubao-seed-1-6-251015",  # è±†åŒ…æ¨¡å‹
            api_key=api_key,
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            timeout=60
        )
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä»€ä¹ˆæ˜¯AIã€‚"}
        ]
        
        logger.info("å‘é€æµ‹è¯•æ¶ˆæ¯...")
        response = llm_provider.generate(messages, temperature=0.7)
        
        print("âœ“ LLMè¿æ¥æˆåŠŸï¼")
        print(f"å›ç­”: {response}")
        print_separator()
        return True
        
    except Exception as e:
        logger.error(f"âœ— LLMè¿æ¥å¤±è´¥: {str(e)}", exc_info=True)
        return False


def test_comment_cleaner():
    """æµ‹è¯•2ï¼šè¯„è®ºæ¸…æ´—æ™ºèƒ½ä½“"""
    print_separator("æµ‹è¯•2ï¼šè¯„è®ºæ¸…æ´—æ™ºèƒ½ä½“æµ‹è¯•")
    
    try:
        api_key = load_ark_api()
        
        # åˆ›å»ºLLMæä¾›è€…
        llm_provider = OpenAICompatibleProvider(
            model_name="doubao-seed-1-6-251015",
            api_key=api_key,
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            timeout=60
        )
        
        # åˆ›å»ºæ•°æ®å·¥å‚æµæ°´çº¿ï¼ˆä¸å¯ç”¨éªŒè¯ï¼‰
        pipeline = DataFactoryPipeline(
            llm_provider=llm_provider,
            enable_verification=False
        )
        
        # åˆ›å»ºæµ‹è¯•è¯„è®º
        test_reviews = create_test_reviews()
        
        # å¤„ç†ç¬¬ä¸€æ¡è¯„è®º
        review = test_reviews[0]
        print(f"åŸå§‹è¯„è®º: {review.content}\n")
        
        logger.info("å¼€å§‹æ¸…æ´—è¯„è®º...")
        result = pipeline.cleaner.process(review.content)
        
        print("âœ“ è¯„è®ºæ¸…æ´—æˆåŠŸï¼")
        print(f"\næ¸…æ´—ç»“æœ:")
        print(f"  - äº‹å®å†…å®¹: {result.factual_content}")
        print(f"  - æƒ…ç»ªå¼ºåº¦: {result.emotional_intensity}")
        print(f"  - åŸå§‹è¯„è®ºé•¿åº¦: {len(review.content)} å­—ç¬¦")
        print(f"  - æ¸…æ´—åé•¿åº¦: {len(result.factual_content)} å­—ç¬¦")
        print_separator()
        return True
        
    except Exception as e:
        logger.error(f"âœ— è¯„è®ºæ¸…æ´—å¤±è´¥: {str(e)}", exc_info=True)
        return False


def test_full_pipeline():
    """æµ‹è¯•3ï¼šå®Œæ•´æµæ°´çº¿"""
    print_separator("æµ‹è¯•3ï¼šå®Œæ•´æ•°æ®å·¥å‚æµæ°´çº¿æµ‹è¯•")
    
    try:
        api_key = load_ark_api()
        
        # åˆ›å»ºLLMæä¾›è€…
        llm_provider = OpenAICompatibleProvider(
            model_name="doubao-seed-1-6-251015",
            api_key=api_key,
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            timeout=60
        )
        
        # åˆ›å»ºæ•°æ®å·¥å‚æµæ°´çº¿ï¼ˆä¸å¯ç”¨éªŒè¯ï¼‰
        del_agent_root = Path(__file__).resolve().parents[1]
        slang_dict_path = del_agent_root / "data" / "slang_dict.json"

        pipeline = DataFactoryPipeline(
            llm_provider=llm_provider,
            enable_verification=False,
            slang_dict_storage="json",
            slang_dict_path=str(slang_dict_path)
        )
        
        # åˆ›å»ºæµ‹è¯•è¯„è®º
        test_reviews = create_test_reviews()
        
        # å¤„ç†æ‰€æœ‰è¯„è®º
        results = []
        for i, review in enumerate(test_reviews, 1):
            print(f"\nå¤„ç†è¯„è®º {i}/{len(test_reviews)}...")
            print(f"åŸå§‹å†…å®¹: {review.content[:50]}...")
            
            try:
                result = pipeline.process_raw_review(review)
                results.append(result)
                
                print(f"âœ“ å¤„ç†æˆåŠŸ")
                print(f"  - å¯¼å¸ˆID: {result.mentor_id}")
                print(f"  - ç»´åº¦: {result.dimension}")
                print(f"  - æƒé‡è¯„åˆ†: {result.weight_score:.2f}")
                
            except Exception as e:
                logger.error(f"âœ— å¤„ç†å¤±è´¥: {str(e)}")
                continue
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print_separator()
        print(f"æµæ°´çº¿ç»Ÿè®¡:")
        print(f"  - æ€»è¯„è®ºæ•°: {len(test_reviews)}")
        print(f"  - æˆåŠŸå¤„ç†: {len(results)}")
        print(f"  - å¤±è´¥æ•°é‡: {len(test_reviews) - len(results)}")
        print(f"  - æˆåŠŸç‡: {len(results)/len(test_reviews)*100:.1f}%")
        
        # æ˜¾ç¤ºé»‘è¯è¯å…¸
        print(f"\né»‘è¯è¯å…¸:")
        slang_dict = pipeline.decoder.dictionary_store.get_all()
        for slang, definition in slang_dict.items():
            print(f"  - '{slang}': {definition}")
        
        # ä¿å­˜ç»“æœ
        output_dir = Path("./output")
        output_dir.mkdir(exist_ok=True)
        
        output_file = output_dir / f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(
                [result.model_dump() for result in results],
                f,
                ensure_ascii=False,
                indent=2,
                default=str
            )
        
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print_separator()
        return True
        
    except Exception as e:
        logger.error(f"âœ— æµæ°´çº¿æµ‹è¯•å¤±è´¥: {str(e)}", exc_info=True)
        return False


def main():
    """ä¸»å‡½æ•°"""
    print_separator("AI Data Factory - è±†åŒ…æ¨¡å‹çœŸå®æµ‹è¯•")
    print("ä½¿ç”¨ç«å±±å¼•æ“ARK API")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print_separator()
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("åŸºç¡€LLMè¿æ¥", test_basic_llm),
        ("è¯„è®ºæ¸…æ´—æ™ºèƒ½ä½“", test_comment_cleaner),
        ("å®Œæ•´æµæ°´çº¿", test_full_pipeline),
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            logger.error(f"æµ‹è¯• '{test_name}' å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            results[test_name] = False
    
    # æ‰“å°æµ‹è¯•æ€»ç»“
    print_separator("æµ‹è¯•æ€»ç»“")
    all_passed = True
    for test_name, passed in results.items():
        status = "âœ“ é€šè¿‡" if passed else "âœ— å¤±è´¥"
        print(f"{status} - {test_name}")
        if not passed:
            all_passed = False
    
    print_separator()
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ã€‚")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
