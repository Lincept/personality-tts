#!/usr/bin/env python3
"""
backend_main.py - åç«¯æ‰¹é‡å¤„ç†å…¥å£

åŠŸèƒ½ï¼š
1. åŠ è½½ data/professors ä¸­çš„è¯„è®ºæ•°æ®
2. é€šè¿‡ DataFactoryPipeline å¤„ç†å¹¶å‹ç¼©æˆçŸ¥è¯†
3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
4. éªŒè¯å’Œå±•ç¤ºå­˜å‚¨ç»“æœ

ä½¿ç”¨æ–¹å¼ï¼š
    python backend_main.py --help              # æŸ¥çœ‹å¸®åŠ©
    python backend_main.py process --limit 5   # å¤„ç†5ä¸ªæ–‡ä»¶
    python backend_main.py show --limit 10     # å±•ç¤º10æ¡å­˜å‚¨è®°å½•
    python backend_main.py stats               # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯

ç¯å¢ƒå˜é‡ï¼ˆ.envï¼‰ï¼š
    BACKEND_TRACE_ENABLED=true/false   # æ§åˆ¶è·Ÿè¸ªè¾“å‡º
    BACKEND_VERBOSE=true/false         # æ§åˆ¶è¯¦ç»†æ—¥å¿—
"""

import os
import sys
import json
import asyncio
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.absolute()
if str(PROJECT_ROOT.parent) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT.parent))

# åŠ è½½ .env æ–‡ä»¶
from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / ".env")

# ä»ç¯å¢ƒå˜é‡è¯»å–æ ‡å¿—ä½
def _env_bool(key: str, default: bool = False) -> bool:
    """ä»ç¯å¢ƒå˜é‡è¯»å–å¸ƒå°”å€¼"""
    val = os.getenv(key, "").lower()
    if val in ("true", "1", "yes", "on"):
        return True
    elif val in ("false", "0", "no", "off"):
        return False
    return default

ENV_TRACE_ENABLED = _env_bool("BACKEND_TRACE_ENABLED", False)
ENV_VERBOSE = _env_bool("BACKEND_VERBOSE", False)

# ç¦ç”¨ loggingï¼ˆä½¿ç”¨ print + æ ‡å¿—ä½æ§åˆ¶è¾“å‡ºï¼‰
logging.disable(logging.CRITICAL)

from del_agent.models.schemas import RawReview, StructuredKnowledgeNode
from del_agent.backend.factory import DataFactoryPipeline
from del_agent.core.llm_adapter import OpenAICompatibleProvider
from del_agent.storage.vector_store import VectorStore, create_vector_store
from del_agent.utils.config import ConfigManager


class BackendProcessor:
    """åç«¯æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(
        self,
        data_dir: str,
        config_path: Optional[str] = None,
        trace_enabled: Optional[bool] = None,
        enable_verification: bool = False
    ):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            trace_enabled: æ˜¯å¦å¯ç”¨è·Ÿè¸ªè¾“å‡ºï¼ˆNoneåˆ™ä».envè¯»å–ï¼‰
            enable_verification: æ˜¯å¦å¯ç”¨æ ¸éªŒ
        """
        self.data_dir = Path(data_dir)
        # ä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        self.trace_enabled = trace_enabled if trace_enabled is not None else ENV_TRACE_ENABLED
        self.enable_verification = enable_verification
        self.verbose = ENV_VERBOSE
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
        
        # å¤„ç†ç»Ÿè®¡
        self.stats = {
            "total_files": 0,
            "total_reviews": 0,
            "processed_reviews": 0,
            "stored_records": 0,
            "failed_reviews": 0,
            "start_time": None,
            "end_time": None
        }
    
    def _load_config(self, config_path: Optional[str]) -> ConfigManager:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_path:
            config_file = config_path
        else:
            config_file = str(PROJECT_ROOT / "config" / "settings.yaml")
        
        return ConfigManager(config_file)
    
    def _init_components(self):
        """åˆå§‹åŒ–å¤„ç†ç»„ä»¶"""
        # åˆ›å»º LLM Provider
        llm_config = self.config.get_llm_config('doubao')
        if not llm_config.api_key:
            raise RuntimeError("æœªæ‰¾åˆ°è±†åŒ… API Keyï¼ˆè¯·è®¾ç½® ARK_API_KEY æˆ– DOBAO_API_KEYï¼‰")
        
        self.llm_provider = OpenAICompatibleProvider(
            model_name=llm_config.model_name,
            api_key=llm_config.api_key,
            base_url=llm_config.base_url,
            timeout=llm_config.timeout,
            api_secret=getattr(llm_config, "api_secret", None)
        )
        self._verbose(f"LLM Provider initialized: {llm_config.model_name}")
        
        # åˆ›å»º Memory Manager å’Œ Vector Store
        mem_config = (
            self.config.get_global_config("mem0_config")
            or self.config.get_global_config("memory")
            or {}
        )
        self.vector_store = create_vector_store(mem_config)
        self._verbose(f"Vector Store enabled: {self.vector_store.enabled}")
        
        # åˆ›å»ºæ•°æ®å·¥å‚
        self.pipeline = DataFactoryPipeline(
            llm_provider=self.llm_provider,
            enable_verification=self.enable_verification,
            vector_store=self.vector_store,
            trace_backend=self.trace_enabled,
            trace_print=self._trace_print
        )
        self._verbose("DataFactoryPipeline initialized")
        
        self._trace("BackendProcessor initialized")
    
    def _trace_print(self, msg: str):
        """è·Ÿè¸ªè¾“å‡º"""
        if self.trace_enabled:
            print(msg)
    
    def _trace(self, msg: str):
        """ç®€åŒ–çš„è·Ÿè¸ªè¾“å‡º"""
        if self.trace_enabled:
            print(f"[BACKEND_MAIN] {msg}")
    
    def _verbose(self, msg: str):
        """è¯¦ç»†æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(f"[VERBOSE] {msg}")
    
    def load_data_files(self, limit: Optional[int] = None) -> List[Path]:
        """
        åŠ è½½æ•°æ®æ–‡ä»¶åˆ—è¡¨
        
        Args:
            limit: æœ€å¤§æ–‡ä»¶æ•°é‡é™åˆ¶
        
        Returns:
            æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        # è·å–æ‰€æœ‰ JSON æ–‡ä»¶
        files = sorted(self.data_dir.glob("*.json"))
        
        if limit and limit > 0:
            files = files[:limit]
        
        self.stats["total_files"] = len(files)
        self._trace(f"Found {len(files)} data files")
        
        return files
    
    def parse_data_file(self, file_path: Path) -> List[RawReview]:
        """
        è§£æå•ä¸ªæ•°æ®æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            RawReview åˆ—è¡¨
        """
        reviews = []
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # æå–æ•™æˆä¿¡æ¯
            professor_info = data.get("input", {})
            sha1 = data.get("sha1", file_path.stem)
            
            # å¤„ç†è¯„è®ºåˆ—è¡¨
            review_list = data.get("data", {}).get("reviews", [])
            
            for review in review_list:
                content = review.get("description", "")
                if not content or not content.strip():
                    continue
                
                # æ„å»ºå…ƒæ•°æ®
                source_metadata = {
                    "sha1": sha1,
                    "professor": professor_info.get("professor", ""),
                    "university": professor_info.get("university", ""),
                    "department": professor_info.get("department", ""),
                    "review_id": review.get("id", ""),
                    "academic_score": review.get("academic", 0),
                    "funding_score": review.get("researchFunding", 0),
                    "relationship_score": review.get("studentProfRelation", 0),
                    "salary_score": review.get("studentSalary", 0),
                    "worktime_score": review.get("workingTime", 0),
                    "job_score": review.get("jobPotential", 0),
                    "anonymous": review.get("anonymous", True)
                }
                
                # è·å–æ—¶é—´æˆ³
                created_at = review.get("created_at", "")
                try:
                    timestamp = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
                except (ValueError, AttributeError):
                    timestamp = datetime.now()
                
                raw_review = RawReview(
                    content=content,
                    source_metadata=source_metadata,
                    timestamp=timestamp
                )
                reviews.append(raw_review)
            
            self._trace(f"Parsed {len(reviews)} reviews from {file_path.name}")
            
        except Exception as e:
            logger.error(f"Failed to parse {file_path}: {e}")
            self._trace(f"[ERROR] Parse failed: {file_path.name} - {e}")
        
        return reviews
    
    async def process_reviews(
        self,
        reviews: List[RawReview],
        store_results: bool = True
    ) -> List[StructuredKnowledgeNode]:
        """
        å¤„ç†è¯„è®ºåˆ—è¡¨
        
        Args:
            reviews: è¯„è®ºåˆ—è¡¨
            store_results: æ˜¯å¦å­˜å‚¨ç»“æœ
        
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        for i, review in enumerate(reviews, 1):
            try:
                self._trace(f"Processing review {i}/{len(reviews)}...")
                
                # å¤„ç†å•æ¡è¯„è®º
                node = await self.pipeline.process_raw_review(review)
                results.append(node)
                self.stats["processed_reviews"] += 1
                
                # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                if store_results and self.vector_store.enabled:
                    success = self._store_knowledge_node(node, review.source_metadata)
                    if success:
                        self.stats["stored_records"] += 1
                
                # è¾“å‡ºå¤„ç†ç»“æœæ‘˜è¦
                self._trace(
                    f"  âœ“ {node.mentor_id} | {node.dimension} | "
                    f"weight={node.weight_score:.2f} | "
                    f"fact={node.fact_content[:50]}..."
                )
                
            except Exception as e:
                logger.error(f"Failed to process review {i}: {e}")
                self._trace(f"  âœ— Error: {e}")
                self.stats["failed_reviews"] += 1
        
        return results
    
    def _store_knowledge_node(
        self,
        node: StructuredKnowledgeNode,
        source_metadata: Dict[str, Any]
    ) -> bool:
        """
        å­˜å‚¨çŸ¥è¯†èŠ‚ç‚¹åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            node: çŸ¥è¯†èŠ‚ç‚¹
            source_metadata: æ¥æºå…ƒæ•°æ®
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # æ„å»ºå­˜å‚¨å†…å®¹
        content = (
            f"æ•™æˆ: {source_metadata.get('professor', 'Unknown')}\n"
            f"å­¦æ ¡: {source_metadata.get('university', 'Unknown')}\n"
            f"é™¢ç³»: {source_metadata.get('department', 'Unknown')}\n"
            f"ç»´åº¦: {node.dimension}\n"
            f"å†…å®¹: {node.fact_content}\n"
            f"æƒé‡: {node.weight_score:.2f}\n"
            f"æ ‡ç­¾: {', '.join(node.tags)}"
        )
        
        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            "mentor_id": node.mentor_id,
            "dimension": node.dimension,
            "weight_score": node.weight_score,
            "tags": node.tags,
            "original_nuance": node.original_nuance,
            "sha1": source_metadata.get("sha1", ""),
            "professor": source_metadata.get("professor", ""),
            "university": source_metadata.get("university", ""),
            "department": source_metadata.get("department", ""),
            "stored_at": datetime.now().isoformat()
        }
        
        # ä½¿ç”¨ mentor_id ä½œä¸º user_id åˆ†ç»„å­˜å‚¨
        user_id = source_metadata.get("sha1", "default")
        
        return self.vector_store.insert(
            content=content,
            user_id=user_id,
            metadata=metadata,
            kind="fact"
        )
    
    async def run_batch_process(
        self,
        limit: Optional[int] = None,
        store_results: bool = True
    ) -> Dict[str, Any]:
        """
        è¿è¡Œæ‰¹é‡å¤„ç†
        
        Args:
            limit: æœ€å¤§æ–‡ä»¶æ•°é‡é™åˆ¶
            store_results: æ˜¯å¦å­˜å‚¨ç»“æœ
        
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        self.stats["start_time"] = datetime.now()
        
        print("=" * 60)
        print("åç«¯æ‰¹é‡å¤„ç†å¼€å§‹")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®æ–‡ä»¶
        files = self.load_data_files(limit)
        print(f"\nğŸ“ å·²åŠ è½½ {len(files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        # 2. è§£æå¹¶å¤„ç†æ¯ä¸ªæ–‡ä»¶
        all_nodes = []
        
        for i, file_path in enumerate(files, 1):
            print(f"\n[{i}/{len(files)}] å¤„ç†æ–‡ä»¶: {file_path.name}")
            print("-" * 40)
            
            # è§£ææ–‡ä»¶
            reviews = self.parse_data_file(file_path)
            self.stats["total_reviews"] += len(reviews)
            
            if not reviews:
                print("  âš ï¸  æ²¡æœ‰è¯„è®ºæ•°æ®")
                continue
            
            # å¤„ç†è¯„è®º
            nodes = await self.process_reviews(reviews, store_results)
            all_nodes.extend(nodes)
        
        self.stats["end_time"] = datetime.now()
        
        # 3. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_summary()
        
        return self.stats
    
    def _print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        duration = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
        
        print("\n" + "=" * 60)
        print("å¤„ç†å®Œæˆ - ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        print(f"  ğŸ“ æ–‡ä»¶æ€»æ•°:     {self.stats['total_files']}")
        print(f"  ğŸ“ è¯„è®ºæ€»æ•°:     {self.stats['total_reviews']}")
        print(f"  âœ… æˆåŠŸå¤„ç†:     {self.stats['processed_reviews']}")
        print(f"  âŒ å¤„ç†å¤±è´¥:     {self.stats['failed_reviews']}")
        print(f"  ğŸ’¾ å­˜å‚¨è®°å½•:     {self.stats['stored_records']}")
        print(f"  â±ï¸  æ€»è€—æ—¶:       {duration:.2f}s")
        
        if self.stats["processed_reviews"] > 0:
            avg_time = duration / self.stats["processed_reviews"]
            print(f"  ğŸ“Š å¹³å‡è€—æ—¶:     {avg_time:.2f}s/æ¡")
        
        print("=" * 60)
    
    def show_stored_records(
        self,
        user_id: Optional[str] = None,
        query: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        å±•ç¤ºå­˜å‚¨çš„è®°å½•
        
        Args:
            user_id: ç”¨æˆ·IDè¿‡æ»¤
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›æ•°é‡é™åˆ¶
        
        Returns:
            è®°å½•åˆ—è¡¨
        """
        if not self.vector_store.enabled:
            print("âš ï¸  å‘é‡å­˜å‚¨æœªå¯ç”¨")
            return []
        
        print("\n" + "=" * 60)
        print("å­˜å‚¨è®°å½•å±•ç¤º")
        print("=" * 60)
        
        if query:
            # æœç´¢æ¨¡å¼
            print(f"ğŸ” æœç´¢: '{query}'")
            records = self.vector_store.search(
                query=query,
                user_id=user_id or "default",
                limit=limit
            )
        else:
            # è·å–æ‰€æœ‰è®°å½•
            print(f"ğŸ“‹ è·å–æ‰€æœ‰è®°å½• (user_id={user_id or 'default'})")
            records = self.vector_store.get_all(user_id=user_id or "default")
            records = records[:limit] if records else []
        
        if not records:
            print("  æ²¡æœ‰æ‰¾åˆ°è®°å½•")
            return []
        
        print(f"\næ‰¾åˆ° {len(records)} æ¡è®°å½•:\n")
        
        for i, record in enumerate(records, 1):
            print(f"--- è®°å½• {i} ---")
            print(f"ID: {record.id or 'N/A'}")
            print(f"å†…å®¹:\n{record.content[:200]}...")
            if record.metadata:
                print(f"å…ƒæ•°æ®: {json.dumps(record.metadata, ensure_ascii=False, indent=2)[:200]}...")
            print()
        
        print("=" * 60)
        
        return [
            {
                "id": r.id,
                "content": r.content,
                "score": r.score,
                "metadata": r.metadata
            }
            for r in records
        ]


def setup_logging(verbose: bool = False):
    """é…ç½®æ—¥å¿—"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="åç«¯æ‰¹é‡å¤„ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python backend_main.py process --limit 5          # å¤„ç†å‰5ä¸ªæ–‡ä»¶
  python backend_main.py process --trace            # å¤„ç†å¹¶è¾“å‡ºè·Ÿè¸ªä¿¡æ¯
  python backend_main.py show --limit 10            # å±•ç¤º10æ¡å­˜å‚¨è®°å½•
  python backend_main.py show --query "å¼ è€å¸ˆ"      # æœç´¢åŒ…å«"å¼ è€å¸ˆ"çš„è®°å½•
  python backend_main.py stats                       # æŸ¥çœ‹æµæ°´çº¿ç»Ÿè®¡
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")
    
    # process å­å‘½ä»¤
    process_parser = subparsers.add_parser("process", help="æ‰¹é‡å¤„ç†è¯„è®ºæ•°æ®")
    process_parser.add_argument(
        "--limit", "-l", type=int, default=None,
        help="æœ€å¤§å¤„ç†æ–‡ä»¶æ•°é‡"
    )
    process_parser.add_argument(
        "--data-dir", "-d", type=str, 
        default=str(PROJECT_ROOT / "data" / "professors"),
        help="æ•°æ®ç›®å½•è·¯å¾„"
    )
    process_parser.add_argument(
        "--trace", "-t", action="store_true",
        help="å¯ç”¨è·Ÿè¸ªè¾“å‡º"
    )
    process_parser.add_argument(
        "--no-store", action="store_true",
        help="ä¸å­˜å‚¨ç»“æœåˆ°æ•°æ®åº“"
    )
    process_parser.add_argument(
        "--verify", action="store_true",
        help="å¯ç”¨æ ¸éªŒå¾ªç¯"
    )
    
    # show å­å‘½ä»¤
    show_parser = subparsers.add_parser("show", help="å±•ç¤ºå­˜å‚¨çš„è®°å½•")
    show_parser.add_argument(
        "--limit", "-l", type=int, default=10,
        help="è¿”å›è®°å½•æ•°é‡é™åˆ¶"
    )
    show_parser.add_argument(
        "--user-id", "-u", type=str, default=None,
        help="æŒ‰ç”¨æˆ·IDè¿‡æ»¤"
    )
    show_parser.add_argument(
        "--query", "-q", type=str, default=None,
        help="æœç´¢æŸ¥è¯¢"
    )
    show_parser.add_argument(
        "--data-dir", "-d", type=str,
        default=str(PROJECT_ROOT / "data" / "professors"),
        help="æ•°æ®ç›®å½•è·¯å¾„"
    )
    
    # stats å­å‘½ä»¤
    stats_parser = subparsers.add_parser("stats", help="æŸ¥çœ‹æµæ°´çº¿ç»Ÿè®¡")
    stats_parser.add_argument(
        "--data-dir", "-d", type=str,
        default=str(PROJECT_ROOT / "data" / "professors"),
        help="æ•°æ®ç›®å½•è·¯å¾„"
    )
    
    # é€šç”¨å‚æ•°
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--config", "-c", type=str, default=None, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    setup_logging(args.verbose)
    
    if not args.command:
        parser.print_help()
        return
    
    # è·å–æ•°æ®ç›®å½•
    data_dir = getattr(args, "data_dir", str(PROJECT_ROOT / "data" / "professors"))
    
    # ç¡®å®š trace çŠ¶æ€ï¼šå‘½ä»¤è¡Œ --trace ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
    trace_enabled = getattr(args, "trace", False) or ENV_TRACE_ENABLED
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = BackendProcessor(
        data_dir=data_dir,
        config_path=args.config,
        trace_enabled=trace_enabled,
        enable_verification=getattr(args, "verify", False)
    )
    
    # æ‰§è¡Œå‘½ä»¤
    if args.command == "process":
        asyncio.run(processor.run_batch_process(
            limit=args.limit,
            store_results=not args.no_store
        ))
    
    elif args.command == "show":
        processor.show_stored_records(
            user_id=args.user_id,
            query=args.query,
            limit=args.limit
        )
    
    elif args.command == "stats":
        stats = processor.pipeline.get_statistics()
        print("\n" + "=" * 40)
        print("æµæ°´çº¿ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 40)
        for key, value in stats.items():
            print(f"  {key}: {value}")
        print("=" * 40)


if __name__ == "__main__":
    main()
